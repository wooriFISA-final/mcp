import os
import logging
import pandas as pd
import json
import re 
import time 
import glob  
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, Body
from datetime import datetime, date
from dateutil.relativedelta import relativedelta 
from dotenv import load_dotenv, find_dotenv, dotenv_values
from pathlib import Path
from langchain_huggingface import HuggingFaceEndpointEmbeddings 
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


# ------------------------------------------------------------------
# ğŸ¯ [Environment Cleanup Function] RAG ì—°ê²° ì˜¤ì—¼ ë³€ìˆ˜ ì´ˆê¸°í™”
# ------------------------------------------------------------------
def _cleanup_rag_env():
    """Hugging Face Endpoint ì¶©ëŒì„ ìœ ë°œí•  ìˆ˜ ìˆëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if 'HUGGINGFACE_API_URL' in os.environ:
        del os.environ['HUGGINGFACE_API_URL']
        logging.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HUGGINGFACE_API_URL ê°•ì œ ì œê±°ë¨.")
    if 'HF_ENDPOINT' in os.environ:
        del os.environ['HF_ENDPOINT']
        logging.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HF_ENDPOINT ê°•ì œ ì œê±°ë¨.")

# ğŸ¯ [ENV ë¡œë“œ]
load_dotenv(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv("..")) 

# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • ì „ì— í™˜ê²½ ë³€ìˆ˜ ì •ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
_cleanup_rag_env() 

# ğŸ¯ [ENV íŒŒì¼ ê°’ ì§ì ‘ ë¡œë“œ]: ì…¸ í™˜ê²½ ë³€ìˆ˜ì™€ì˜ ì¶©ëŒì„ ë§‰ê¸° ìœ„í•´ íŒŒì¼ ë‚´ìš©ë§Œ ë‹¤ì‹œ ì½ì–´ì˜µë‹ˆë‹¤.
ENV_VALUES = dotenv_values(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv(".."))

# ğŸ¯ [ìš”ì²­ ê²½ë¡œ ë°˜ì˜]
from server.schemas.report_schema import (
    AnalyzeSpendingInput, AnalyzeSpendingOutput, 
    FinalSummaryInput, FinalSummaryOutput, 
    ToolSkippedOutput, PolicyRAGSearchInput, PolicyRAGSearchOutput 
)


logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • (ENV_VALUES ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ) - ì¶©ëŒ ë°©ì§€ ëª©ì 
# ------------------------------------------------------------------
# RAG ë° ì •ì±… ê²€ìƒ‰ì— í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
HF_EMBEDDING_MODEL = ENV_VALUES.get("HF_EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
VECTOR_DB_PATH = ENV_VALUES.get("VECTOR_DB_PATH", './data/faiss_index')
HUGGINGFACEHUB_API_TOKEN = ENV_VALUES.get("HUGGINGFACEHUB_API_TOKEN")



# ğŸš¨ [ì¶”ê°€] ì •ì±… ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
POLICY_DIR = "./data/policy_documents"


router = APIRouter(
    prefix="/report_processing",
    tags=["Report Processing Tools"] 
)

# ------------------------------------------------------------------
# ğŸ¯ [ìƒˆë¡œìš´ ìƒìˆ˜ ì •ì˜] ì •ì±… íŒŒì¼ê³¼ ì ìš© ì›”ì˜ ê·œì¹™ ë§¤í•‘ (YYYYMMDD_policy.pdf)
# ------------------------------------------------------------------
# ì •ì±… ë°°í¬ì¼(YYYYMMDD) ëª©ë¡. ì´ ë‚ ì§œì˜ ì •ì±…ì€ ë‹¤ìŒ ë‹¬ 1ì¼ ë³´ê³ ì„œì— ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
POLICY_FILE_DATES = [
    "20250305",  # 2025ë…„ 4ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20241224",  # 2025ë…„ 1ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240724",  # 2024ë…„ 8ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240626",  # 2024ë…„ 7ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240430",  # 2024ë…„ 5ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230830",  # 2023ë…„ 9ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230621",  # 2023ë…„ 7ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230302",  # 2023ë…„ 4ì›” ë³´ê³ ì„œì— ë°˜ì˜
]

# ------------------------------------------------------------------
# ğŸ¯ [ìˆ˜ì •ëœ í•¨ìˆ˜]: ë³´ê³ ì„œ ì›”ì— í•´ë‹¹í•˜ëŠ” ì •ì±… íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
# ------------------------------------------------------------------
def _find_policy_file_for_report(report_date_str: str) -> Optional[str]:
    """
    ë³´ê³ ì„œ ë‚ ì§œ(YYYY-MM-DD)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, í•´ë‹¹ ì›”ì— ë°˜ì˜í•´ì•¼ í•  ì •ì±… íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    (ì •ì±… ë°œí‘œì¼ ë‹¤ìŒ ë‹¬ 1ì¼ì´ ë³´ê³ ì„œ ì‘ì„±ì¼ì¸ ê²½ìš° í•´ë‹¹ ì •ì±… íŒŒì¼ì„ ì„ íƒ)
    """
    try:
        # ë³´ê³ ì„œê°€ ë°œí–‰ë˜ëŠ” ì›”ì˜ 1ì¼ (ì˜ˆ: 2024-12-01)
        report_month_start = datetime.strptime(report_date_str, "%Y-%m-%d").replace(day=1)

        # ì •ì±… íŒŒì¼ ë‚ ì§œ ëª©ë¡ì„ ì—­ìˆœìœ¼ë¡œ ìˆœíšŒ (ìµœì‹  ì •ì±…ë¶€í„° í™•ì¸)
        for policy_date_str in sorted(POLICY_FILE_DATES, reverse=True):
            policy_date = datetime.strptime(policy_date_str, "%Y%m%d").date()
            
            # ì •ì±… ë°œí‘œì¼ì˜ ë‹¤ìŒ ë‹¬ 1ì¼ (ì •ì±… ë³€ë™ ì‚¬í•­ì´ ë°˜ì˜ë  ëª©í‘œ ë³´ê³ ì„œ ì›”)
            policy_effective_month_start = (datetime(policy_date.year, policy_date.month, 1) + relativedelta(months=1))
            
            # ë§Œì•½ ì •ì±…ì˜ ìœ íš¨ ì‹œì‘ ì›”ì´ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë³´ê³ ì„œ ì›”ê³¼ ê°™ë‹¤ë©´, ì´ ì •ì±…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if policy_effective_month_start == report_month_start:
                filename = f"{policy_date_str}_policy.pdf"
                full_path = os.path.join(POLICY_DIR, filename)
                
                if os.path.exists(full_path):
                    logger.info(f"RAG: {report_date_str[:7]} ë³´ê³ ì„œì— {filename} ì •ì±… íŒŒì¼ ì§€ì •ë¨.")
                    return full_path
                else:
                    logger.warning(f"RAG: ì§€ì •ëœ ì •ì±… íŒŒì¼({filename})ì´ ë””ë ‰í† ë¦¬ì— ì—†ìŠµë‹ˆë‹¤.")
                    return None
        
        logger.info(f"RAG: {report_date_str[:7]} ë³´ê³ ì„œì— ë°˜ì˜í•  ì •ì±… íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í•´ë‹¹ ì›”ì€ ì •ì±… ë³€ë™ ì—†ìŒ)")
        return None
        
    except Exception as e:
        logger.error(f"ì •ì±… íŒŒì¼ ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ TOOL 0] ë²¡í„° DB ì¬êµ¬ì¶• ë° ì—…ë°ì´íŠ¸
# ------------------------------------------------------------------
# @router.post(
#     "/rebuild_vector_db",
#     summary="ì •ì±… ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISS ë²¡í„° DBë¥¼ ì¬êµ¬ì¶•",
#     operation_id="rebuild_vector_db_tool",
#     description="data/policy_documents í´ë”ì˜ ëª¨ë“  PDFë¥¼ ì½ì–´ ë²¡í„° DBë¥¼ ì™„ì „íˆ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.",
#     response_model=dict,
# )
# async def api_rebuild_vector_db() -> dict:
#     """PDF íŒŒì¼ì„ ë¡œë“œ, ë¶„í• , ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    
#     logger.info(f"--- RAG ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘ (Model: {HF_EMBEDDING_MODEL}) ---")
    
#     if not os.path.exists(POLICY_DIR):
#         error_msg = f"âŒ '{POLICY_DIR}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. data/policy_documents í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
#         logger.error(error_msg)
#         return {"tool_name": "rebuild_vector_db_tool", "success": False, "error": error_msg}

#     file_paths = glob.glob(os.path.join(POLICY_DIR, '*.pdf'))
#     if not file_paths:
#         info_msg = f"âœ… policy_documents í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì¶”ê°€í•´ ì£¼ì„¸ìš”."
#         logger.info(info_msg)
#         return {"tool_name": "rebuild_vector_db_tool", "success": True, "message": info_msg}

#     documents = []
#     for file_path in file_paths:
#         loader = PyPDFLoader(file_path)
#         documents.extend(loader.load())

#     custom_separators = [
#         r"\nì œ[0-9]{1,3}ì¥\s",
#         r"\nì œ[0-9]{1,3}ì¡°\s",
#         r"\n[ê°€-í£\d]\.\s?",
#         r"\n\([ê°€-í£\d]{1,2}\)\s?",
#         r"\n",                           
#         " ",
#         ""
#     ]
    
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=1000,  
#         chunk_overlap=50, 
#         separators=custom_separators,
#         keep_separator=True
#     )
#     texts = text_splitter.split_documents(documents)
#     logger.info(f"â¡ï¸ ì´ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ.")
    
#     if not HUGGINGFACEHUB_API_TOKEN:
#         error_msg = "âŒ HUGGINGFACEHUB_API_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
#         logger.error(error_msg)
#         return {"tool_name": "rebuild_vector_db_tool", "success": False, "error": error_msg}
    
#     try:
#         embeddings = HuggingFaceEndpointEmbeddings(
#             model=HF_EMBEDDING_MODEL, 
#             huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
#         )
#     except Exception as e:
#         error_msg = f"ğŸš¨ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"
#         logger.error(error_msg)
#         return {"tool_name": "rebuild_vector_db_tool", "success": False, "error": error_msg}

#     logger.info(f"ğŸ’¾ FAISS ë²¡í„° DB ìƒì„± ì¤‘... ({len(texts)}ê°œ ì²­í¬)")
#     batch_size = 32
#     sleep_time = 3
    
#     if not texts:
#         return {"tool_name": "rebuild_vector_db_tool", "success": True, "message": "ê²½ê³ : ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—†ì–´ DB êµ¬ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤."}
        
#     first_batch = texts[:batch_size]
#     remaining_texts = texts[batch_size:]

#     try:
#         db = FAISS.from_documents(first_batch, embeddings)
#     except Exception as e:
#         error_msg = f"ğŸš¨ DB ì´ˆê¸° ìƒì„± ì‹¤íŒ¨: {e}"
#         logger.error(error_msg)
#         return {"tool_name": "rebuild_vector_db_tool", "success": False, "error": error_msg}

#     total_processed = len(first_batch)
    
#     for i in range(0, len(remaining_texts), batch_size):
#         batch = remaining_texts[i:i + batch_size]
#         logger.info(f"   -- API ìš”ì²­ ì§€ì—° ({sleep_time}ì´ˆ ëŒ€ê¸°) --")
#         time.sleep(sleep_time)
        
#         try:
#             db.add_documents(batch)
#             total_processed += len(batch)
#             logger.info(f"   -> {total_processed} / {len(texts)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ.")
#         except Exception as e:
#             error_msg = f"ğŸš¨ ì„ë² ë”© ì˜¤ë¥˜ ë°œìƒ: {e}. DB êµ¬ì¶•ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
#             logger.error(error_msg)
#             return {"tool_name": "rebuild_vector_db_tool", "success": False, "error": error_msg}

#     Path(VECTOR_DB_PATH).mkdir(parents=True, exist_ok=True)
#     db.save_local(VECTOR_DB_PATH) 

#     info_msg = f"--- âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ --- (ì €ì¥ ê²½ë¡œ: {VECTOR_DB_PATH})"
#     logger.info(info_msg)
#     return {"tool_name": "rebuild_vector_db_tool", "success": True, "message": info_msg}


# ------------------------------------------------------------------
# ğŸ¯ [ì •ì±… ì„¹ì…˜ ì •ì˜] RAG ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ì„¹ì…˜ ëª©ë¡
# ------------------------------------------------------------------
# 1ì¥ì€ ë¬¸ì„œ ì „ì²´ ê°œì • ì´ë ¥ë§Œ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³ , 2ì¥ë¶€í„° ê²€ìƒ‰
POLICY_SECTIONS_TO_CHECK = [
    "2ì¥ ë‚˜. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ë‹´ë³´ì¸ì •ë¹„ìœ¨(LTV) ë³€ë™ ë° íŠ¹ë¡€ ì ìš©",
    "3ì¥ ë‹¤. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ì´ë¶€ì±„ìƒí™˜ë¹„ìœ¨(DTI) ì ìš© ë° ë°°ì œ ê¸°ì¤€",
    "4ì¥ ë¼. ê³ ì•¡ ê°€ê³„ëŒ€ì¶œ DSR ì ìš© ê¸°ì¤€ ë° ì˜ˆì™¸ ì‚¬í•­",
    "5ì¥ ë§ˆ. ì£¼íƒê´€ë ¨ ë‹´ë³´ëŒ€ì¶œ ì·¨ê¸‰ ê´€ë ¨ ìœ ì˜ì‚¬í•­ ë° íŠ¹ë¡€ ëŒ€ì¶œ ì‹ ì„¤"
]


# ------------------------------------------------------------------
# ğŸ¯ RAG ê²€ìƒ‰ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë‹¨ì¼ íŒŒì¼ í•„í„°ë§ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •)
# ------------------------------------------------------------------
def _rag_similarity_search(query: str, k: int = 5, required_sources: Optional[List[str]] = None) -> str:
    """FAISS DBë¥¼ ë¡œë“œí•˜ì—¬ ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì§€ì •ëœ ì†ŒìŠ¤ íŒŒì¼ ëª©ë¡ì—ì„œë§Œ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""

    if not HUGGINGFACEHUB_API_TOKEN:
        return "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: HUGGINGFACEHUB_API_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
    current_model = HF_EMBEDDING_MODEL 
    logger.info(f"RAG: ì„ë² ë”© ëª¨ë¸ {current_model} ì‚¬ìš©.")

    try:
        embeddings = HuggingFaceEndpointEmbeddings(
            model=current_model,
            huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
        )
        
        db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        
        found_chunks = db.similarity_search(query, k=k * 4) 
        
        logger.info(f"RAG: ê²€ìƒ‰ì–´ '{query}'ë¡œ {len(found_chunks)}ê°œ ì²­í¬ ë°œê²¬ (required_sources: {required_sources})")
        
        context = []
        filtered_count = 0
        
        for idx, chunk in enumerate(found_chunks):
            source = chunk.metadata.get("source", "ì¶œì²˜ ë¯¸ìƒ")
            
            # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ ì²­í¬ì˜ source ì¶œë ¥
            if idx < 5:
                logger.info(f"RAG: ì²­í¬ {idx} - source: '{source}'")
            
            is_valid_source = not required_sources or any(req_src in source for req_src in required_sources)
            
            if not is_valid_source:
                if idx < 5:
                    logger.info(f"RAG: ì²­í¬ {idx} - í•„í„°ë§ë¨ (source ë¶ˆì¼ì¹˜)")
                continue

            if filtered_count < k:
                context.append(f"[ì¶œì²˜: {source}]\n{chunk.page_content}")
                filtered_count += 1
                if idx < 5:
                    logger.info(f"RAG: ì²­í¬ {idx} - í¬í•¨ë¨!")
            else:
                break

        logger.info(f"RAG: ìµœì¢… {filtered_count}ê°œ ì²­í¬ ë°˜í™˜ (ëª©í‘œ: {k}ê°œ)")
        
        if not context:
            source_info = f"ë¬¸ì„œ ëª©ë¡: {required_sources}" if required_sources else "ëª¨ë“  ë¬¸ì„œ"
            return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•´ {source_info}ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
        return "\n---\n".join(context)
    
    except Exception as e:
        logger.error(f"RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}", exc_info=True)
        return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {type(e).__name__} - {e}"


# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ í•¨ìˆ˜]: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ 100% íƒì§€ (ê°œì •/ì‹ ì„¤ ìœ ì—°ì„± ê°•í™”)
# ------------------------------------------------------------------
def _find_policies_by_marker_regex(context: str, target_date: Optional[str] = None) -> List[Dict[str, str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ <ì‹ ì„¤ YYYY.M.D.> ë˜ëŠ” <ê°œì • YYYY.M.D.> ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì •ì±… êµ¬ë¬¸ì„ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ ë° ì •ê·œí™”.
    
    Args:
        context: RAG ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        target_date: í•„í„°ë§í•  ëª©í‘œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì§€ì • ì‹œ í•´ë‹¹ ë‚ ì§œì˜ ë³€ê²½ì‚¬í•­ë§Œ ë°˜í™˜
    """
    
    context_clean = re.sub(r'\[ì¶œì²˜:.*?\.pdf\]', '', context, flags=re.DOTALL)
    context_clean = re.sub(r'---\n', '', context_clean, flags=re.DOTALL)
    
    # ì •ê·œí‘œí˜„ì‹: ì¡°í•­ ë²ˆí˜¸ + ë‚´ìš© + <ì‹ ì„¤/ê°œì • ë‚ ì§œ> íŒ¨í„´ ì°¾ê¸°
    # ì˜ˆ: "21.(ì„ì°¨ë³´ì¦ê¸ˆë°˜í™˜ëª©ì ...) <ê°œì • 2024.7.24., 2024.12.24.>"
    # <ë³„í‘œ6><ì‹ ì„¤...> ê°™ì€ ë¬¸ì„œ ì „ì²´ ê°œì • ì´ë ¥ì€ ì œì™¸
    regex = r"(\d{1,3}\.[\s\S]{10,1000}?)<\s*(ì‹ ì„¤|ê°œì •)\s*([^>]+)>"
    
    matches = re.findall(regex, context_clean, re.DOTALL) 
    
    logger.info(f"RAG: ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­ ê²°ê³¼ {len(matches)}ê°œ ë°œê²¬")
    
    extracted_changes = []
    
    for policy_text, change_type, dates_str in matches:
        # <ë³„í‘œX> íŒ¨í„´ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸
        if re.search(r'<ë³„í‘œ\d+>', policy_text):
            logger.info(f"RAG: <ë³„í‘œ> íŒ¨í„´ ë°œê²¬ìœ¼ë¡œ ì œì™¸")
            continue
        
        # ë‚ ì§œ ë¬¸ìì—´ì—ì„œ ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ
        date_pattern = r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        all_dates = re.findall(date_pattern, dates_str)
        
        if not all_dates:
            logger.warning(f"RAG: ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ - dates_str: '{dates_str}'")
            continue
        
        # ê°€ì¥ ìµœì‹  ë‚ ì§œ ì°¾ê¸° (ë§ˆì§€ë§‰ ë‚ ì§œê°€ ë³´í†µ ìµœì‹ )
        latest_date_tuple = all_dates[-1]
        year, month, day = latest_date_tuple
        
        try:
            effective_date = datetime(int(year), int(month), int(day)).strftime("%Y-%m-%d")
        except ValueError:
            logger.warning(f"RAG: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ - year: {year}, month: {month}, day: {day}")
            continue
        
        logger.info(f"RAG: ë°œê²¬ëœ ë³€ê²½ì‚¬í•­ - ë‚ ì§œ: {effective_date}, íƒ€ì…: {change_type}, ì¡°í•­: {policy_text[:50]}...")
        
        # target_dateê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ í¬í•¨
        if target_date:
            if effective_date != target_date:
                logger.info(f"RAG: ë‚ ì§œ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸ - effective_date: {effective_date}, target_date: {target_date}")
                continue
            else:
                logger.info(f"RAG: ë‚ ì§œ ì¼ì¹˜! í¬í•¨ - {effective_date}")
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        normalized_text = policy_text.strip()
        normalized_text = re.sub(r'\s{2,}', ' ', normalized_text)
        
        # ë§ˆì»¤ ì¶”ê°€
        full_text = f"{normalized_text} <{change_type} {dates_str}>"
        
        extracted_changes.append({
            "effective_date": effective_date,
            "policy_text": full_text
        })

    logger.info(f"RAG: ìµœì¢… ì¶”ì¶œëœ ë³€ê²½ì‚¬í•­ {len(extracted_changes)}ê°œ (target_date: {target_date})")
    return extracted_changes



# ------------------------------------------------------------------
# ğŸ¯ [REMOVED]: _generate_final_report_from_structured_data
# This function has been removed. The Agent will now handle policy report generation.
# ------------------------------------------------------------------


# ==============================================================================
# ë…ë¦½ Tool 1: ì†Œë¹„ ë°ì´í„° ë¶„ì„ ë° êµ°ì§‘ ìƒì„±
# ==============================================================================
@router.post(
    "/analyze_user_spending",
    summary="ì›”ë³„ ì†Œë¹„ ë°ì´í„° ë¹„êµ ë¶„ì„ ë° êµ°ì§‘ ìƒì„±",
    operation_id="analyze_user_spending_tool", 
    description="ë‘ ë‹¬ì¹˜ ì†Œë¹„ ë°ì´í„°(DataFrame Records)ë¥¼ ë°›ì•„ ì´ ì§€ì¶œ, Top 5 ì¹´í…Œê³ ë¦¬ë¥¼ ë¹„êµ ë¶„ì„í•˜ê³ , êµ°ì§‘ ë³„ëª…ê³¼ ì¡°ì–¸ì„ LLMì„ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_spending(
    consume_records: List[Dict[str, Any]] = Body(..., embed=True),
    member_data: Dict[str, Any] = Body(..., embed=False)
) -> dict:
    """ì†Œë¹„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""

    if not consume_records or len(consume_records) < 2:
        error_msg = "ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ìµœì†Œ 2ê°œì›” ë°ì´í„° ë¶€ì¡±" if consume_records else "ë¶„ì„í•  ì†Œë¹„ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤."
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": False, 
            "error": error_msg,
            "consume_analysis_summary": {},
            "spend_chart_json": json.dumps({})
        }
    
    try:
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ë° ì •ë ¬
        df_consume = pd.DataFrame(consume_records)
        df_consume['year_and_month'] = pd.to_datetime(df_consume['year_and_month'])
        df_consume = df_consume.sort_values(by='year_and_month', ascending=False)
        
        latest_data = df_consume.iloc[0] # ìµœì‹  ì›” ë°ì´í„°
        previous_data = df_consume.iloc[1]

        total_spend_latest = latest_data.get('total_spend', 0) or 0
        total_spend_prev = previous_data.get('total_spend', 0) or 0
        diff = total_spend_latest - total_spend_prev
        change_rate = (diff / total_spend_prev) * 100 if total_spend_prev else 0
        change_text = f"{diff:+,}ì› ({change_rate:.2f}%) ë³€ë™"

        # ğŸš¨ [ìˆ˜ì •] ì†Œë¶„ë¥˜(CAT2) ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ëŒ€ë¶„ë¥˜(CAT1) ì‚¬ìš©
        cat2_cols = [col for col in latest_data.index if col.startswith('CAT2_')]
        target_cols = cat2_cols if cat2_cols else [col for col in latest_data.index if col.startswith('CAT1_')]
        prefix = 'CAT2_' if cat2_cols else 'CAT1_'
        
        # Top 5 ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ë¶„ì„ í…ìŠ¤íŠ¸ìš© - ì—¬ì „íˆ ëŒ€ë¶„ë¥˜ ê¸°ì¤€ì´ ì¢‹ì„ ìˆ˜ ìˆìœ¼ë‚˜, ì¼ê´€ì„±ì„ ìœ„í•´ target_cols ì‚¬ìš©)
        # ë§Œì•½ ë¶„ì„ í…ìŠ¤íŠ¸ëŠ” ëŒ€ë¶„ë¥˜ë¡œ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ cat1_colsë¥¼ ë³„ë„ë¡œ êµ¬í•´ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” ì°¨íŠ¸ì™€ ì¼ê´€ë˜ê²Œ ì†Œë¶„ë¥˜ê°€ ìˆìœ¼ë©´ ì†Œë¶„ë¥˜ Top 5ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•¨.
        latest_cats = df_consume.iloc[0][target_cols].sort_values(ascending=False).head(5) 
        
        # spend_chart_jsonì„ ìœ„í•œ ì „ì²´ ì¹´í…Œê³ ë¦¬ë³„ ê¸ˆì•¡ ê³„ì‚°
        chart_data_list = []
        for col in target_cols:
            amount = latest_data.get(col, 0) or 0
            if amount > 0:
                # ë¼ë²¨ ì •ì œ: ì ‘ë‘ì‚¬ ì œê±° ë° ì–¸ë”ë°”ë¥¼ ê³µë°±/ìŠ¬ë˜ì‹œë¡œ ë³€í™˜
                label = col.replace(prefix, '').replace('_', ' ').replace(' ', '/') 
                chart_data_list.append({
                    "category": label,
                    "amount": int(amount)
                })
        spend_chart_json = json.dumps(chart_data_list, ensure_ascii=False)
        
        # consume_analysis_summary êµ¬ì„± (Agentê°€ ì‚¬ìš©í•  ë°ì´í„°)
        consume_analysis_summary = {
            'latest_total_spend': int(total_spend_latest),
            'previous_total_spend': int(total_spend_prev),
            'spend_diff': int(diff),
            'change_rate': round(change_rate, 2),
            'total_change_diff': change_text,
            'top_5_categories': [col.replace('CAT1_', '') for col in latest_cats.index],
            'top_5_amounts': [int(latest_cats[col]) for col in latest_cats.index],
            'member_info': member_data
        }
        
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": True, 
            "consume_analysis_summary": consume_analysis_summary,
            "spend_chart_json": spend_chart_json
        }

    except Exception as e:
        logger.error(f"ì†Œë¹„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": str(e)}

    
# ==============================================================================
# ë…ë¦½ Tool 2: ìµœì¢… 3ì¤„ ìš”ì•½ LLM Tool
# ==============================================================================
@router.post(
    "/generate_final_summary",
    summary="ìµœì¢… ë³´ê³ ì„œ 3ì¤„ ìš”ì•½ ìƒì„±",
    operation_id="generate_final_summary_llm", 
    description="í†µí•© ë³´ê³ ì„œ ë³¸ë¬¸ì„ ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_generate_final_summary(report_content: str = Body(..., embed=True)) -> dict:
    """
    [DEPRECATED] This tool now returns the report content as-is. 
    The Agent will handle summarization internally instead of calling this tool.
    """
    
    return {
        "tool_name": "generate_final_summary_llm", 
        "success": True, 
        "report_content": report_content,
        "message": "ì´ ë„êµ¬ëŠ” ë” ì´ìƒ LLMì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Agentê°€ ì§ì ‘ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }



# ==============================================================================
# ë…ë¦½ Tool 3: ì •ì±… ë³€ë™ ìë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„± íˆ´ (ğŸŒŸ ìˆ˜ì • ì™„ë£Œ)
# ==============================================================================
@router.post(
    "/check_and_report_policy_changes",
    summary="ë§¤ì›” ìë™ ì •ì±… ë³€ë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="check_and_report_policy_changes_tool", 
    description="ì‚¬ìš©ì ì…ë ¥ ì—†ì´, ì •ì˜ëœ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ë³€ë™ ì‚¬í•­ì„ í™•ì¸í•˜ê³  êµ¬ì¡°í™”ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_check_policy_changes(
    report_month_str: str = Body(..., embed=True) 
) -> dict:
    """ë§¤ì›” ì •ê¸° ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì •ì±… ë³€ë™ ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""
    
    # ğŸ¯ [í•µì‹¬ ìˆ˜ì •]: ë³´ê³ ì„œ ì›”ì— í•´ë‹¹í•˜ëŠ” ì •ì±… íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤. (ìƒˆë¡œìš´ ë¡œì§ ë°˜ì˜)
    LATEST_POLICY_SOURCE = _find_policy_file_for_report(report_month_str)
    
    if not LATEST_POLICY_SOURCE:
        # ì •ì±… íŒŒì¼ì´ ì—†ìœ¼ë©´ ë³€ë™ ì—†ìŒìœ¼ë¡œ ê°„ì£¼
        report_month = datetime.strptime(report_month_str, "%Y-%m-%d").date().strftime('%Yë…„ %mì›”')
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": True, 
            "policy_changes": [],
            "report_month": report_month,
            "message": f"{report_month} ë³´ê³ ì„œ ê¸°ì¤€, í•´ë‹¹ ì›”ì— ë°˜ì˜í•  ì •ì±… ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."
        }
        
    REQUIRED_SOURCES = [LATEST_POLICY_SOURCE] 
    
    # 1. ğŸ“… ë‚ ì§œ ì²´í¬ ë° ì´ˆê¸° ì„¤ì •
    try:
        report_month = datetime.strptime(report_month_str, "%Y-%m-%d").date()
        
        # ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ
        file_name = Path(LATEST_POLICY_SOURCE).name 
        latest_policy_date_str = file_name.split('_')[0] # 'YYYYMMDD' ì¶”ì¶œ
        latest_policy_date = datetime.strptime(latest_policy_date_str, "%Y%m%d").date()
        
    except ValueError:
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": False, 
            "policy_changes": [],
            "error": "ë³´ê³ ì„œ ì›” í˜•ì‹ ì˜¤ë¥˜"
        }
    
    # ----------------------------------------------------------------------
    # 2. âš™ï¸ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ ì‹¤í–‰ (ì§€ì •ëœ íŒŒì¼ë§Œ ëŒ€ìƒ)
    # ----------------------------------------------------------------------
    full_context_list = []
    K_SEARCH = 15  # ê° ì„¹ì…˜ë‹¹ ê²€ìƒ‰í•  ì²­í¬ ìˆ˜ (7 -> 15ë¡œ ì¦ê°€)
 
    
    for section_query in POLICY_SECTIONS_TO_CHECK:
        rag_context = _rag_similarity_search(
            query=section_query, 
            k=K_SEARCH, 
            required_sources=REQUIRED_SOURCES 
        )

        if "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨" not in rag_context:
            full_context_list.append(rag_context)
        else:
             return {
                "tool_name": "check_and_report_policy_changes_tool", 
                "success": False, 
                "policy_changes": [],
                "error": rag_context
            }

    combined_context = "\n---\n".join(full_context_list)
    
    # ë””ë²„ê¹…: combined_contextì˜ ì¼ë¶€ë¥¼ ì¶œë ¥
    logger.info(f"RAG: combined_context ê¸¸ì´: {len(combined_context)} ë¬¸ì")
    logger.info(f"RAG: combined_context ìƒ˜í”Œ (ì²˜ìŒ 500ì):\n{combined_context[:500]}")
    
    # 3. ğŸ“ ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ RAG ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ ì¶”ì¶œ
    # ì •ì±… íŒŒì¼ ë‚ ì§œë¥¼ target_dateë¡œ ì „ë‹¬í•˜ì—¬ í•´ë‹¹ ë‚ ì§œì˜ ë³€ê²½ì‚¬í•­ë§Œ í•„í„°ë§
    target_policy_date = latest_policy_date.strftime("%Y-%m-%d")
    structured_changes_raw = _find_policies_by_marker_regex(combined_context, target_date=target_policy_date)
    
    # 4. ğŸ¯ ìµœì¢… íŒŒì´ì¬ í•„í„°ë§
    # ğŸš¨ [ìˆ˜ì •]: ì •ì±… íŒŒì¼ì´ ì´ë¯¸ ì„ íƒë˜ì—ˆìœ¼ë¯€ë¡œ, í•´ë‹¹ íŒŒì¼ ë‚´ ëª¨ë“  ì‹ ì„¤/ê°œì • ì‚¬í•­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    structured_changes = structured_changes_raw
    
    # 5. ğŸ§© ì¶”ì¶œ ê²°ê³¼ ì²˜ë¦¬ (ë³€ë™ ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš°)
    if not structured_changes:
        # ì •ì±… íŒŒì¼ì€ ìˆì—ˆìœ¼ë‚˜, í•´ë‹¹ íŒŒì¼ì—ì„œ ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì •ì±… ë³€ë™ ì‚¬í•­ì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": True, 
            "policy_changes": [],
            "report_month": report_month.strftime('%Yë…„ %mì›”'),
            "policy_date": latest_policy_date.strftime('%Yë…„ %mì›” %dì¼'),
            "message": f"{report_month.strftime('%Yë…„ %mì›”')} ë³´ê³ ì„œ ê¸°ì¤€, ìµœì‹  ì •ì±… ë¬¸ì„œ({latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')})ì— ì‹ ì„¤ ë˜ëŠ” ê°œì •ëœ ì •ì±… ë³€ë™ ì‚¬í•­ì€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }

    # 6. ğŸ¯ ìµœì¢… ì•„ì›ƒí’‹ êµ¬ì„± (Agentê°€ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±)
    return {
        "tool_name": "check_and_report_policy_changes_tool", 
        "success": True, 
        "policy_changes": structured_changes,
        "report_month": report_month.strftime('%Yë…„ %mì›”'),
        "policy_date": latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')
    }


# ==============================================================================
# ë…ë¦½ Tool 4: ì†ìµ/ì§„ì²™ë„ ë¶„ì„ (ì™„ì„±)
# ==============================================================================
@router.post(
    "/analyze_investment_profit",
    summary="íˆ¬ì ìƒí’ˆ ì†ìµ/ì§„ì²™ë„ ë¶„ì„",
    operation_id="analyze_investment_profit_tool", 
    description="ì˜ˆê¸ˆ, ì ê¸ˆ, í€ë“œì˜ ìˆ˜ìµë¥ ê³¼ ì§„ì²™ë„ë¥¼ ë¶„ì„í•˜ê³  LLMì„ í†µí•´ ì¡°ì–¸ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_analyze_investment_profit(products: List[Dict[str, Any]] = Body(..., embed=True)) -> dict:
    """ë³´ìœ  ìƒí’ˆ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì†ìµ ë°ì´í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""
    
    if not products:
        return {
            "tool_name": "analyze_investment_profit_tool", 
            "success": True, 
            "total_principal": 0,
            "total_valuation": 0,
            "net_profit": 0,
            "profit_rate": 0.0,
            "products_count": 0,
            "message": "í˜„ì¬ ë³´ìœ  ì¤‘ì¸ íˆ¬ì ìƒí’ˆì´ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆí‚µë‹ˆë‹¤."
        }

    total_principal = 0
    total_valuation = 0
    
    for p in products:
        principal = p.get('total_principal', 0) or 0
        valuation = p.get('current_valuation', 0) or 0
        total_principal += principal
        total_valuation += valuation 

    net_profit = total_valuation - total_principal
    profit_rate = (net_profit / total_principal) * 100 if total_principal else 0
    
    return {
        "tool_name": "analyze_investment_profit_tool", 
        "success": True, 
        "total_principal": int(total_principal),
        "total_valuation": int(total_valuation),
        "net_profit": int(net_profit),
        "profit_rate": round(profit_rate, 2),
        "products_count": len(products)
    }



# ==============================================================================
# ë…ë¦½ Tool 5: ì‚¬ìš©ì í”„ë¡œí•„ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± - ğŸŒŸ ìµœì¢… ì•ˆì •í™”
# ==============================================================================
@router.post(
    "/analyze_user_profile_changes",
    summary="ì‚¬ìš©ì ê°œì¸ ì§€ìˆ˜ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="analyze_user_profile_changes_tool",
    description="ì§ì „ ë³´ê³ ì„œì™€ í˜„ì¬ DBì—ì„œ ì¡°íšŒí•œ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ë¥¼ ë¹„êµí•˜ê³ , LLMì„ í†µí•´ ë³€ë™ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_profile_changes(
    current_data: Dict[str, Any] = Body(..., embed=True),
    previous_data: Dict[str, Any] = Body(..., embed=False)
) -> dict:
    """ì‚¬ìš©ìì˜ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ ë³€ë™ ë°ì´í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""
    
    # 1. ğŸ“Š ë°ì´í„° ë¹„êµ ë° ìš”ì•½
    change_raw_changes = []
    
    # ë¹„êµ ëŒ€ìƒ í•„ë“œ ë¦¬ìŠ¤íŠ¸
    fields_to_compare = [
        ('annual_salary', 'ì—°ë´‰'), 
        ('total_debt', 'ì´ ë¶€ì±„'), 
        ('credit_score', 'ì‹ ìš© ì ìˆ˜')
    ]

    is_first_report = all(v == 0 for k, v in previous_data.items() if k in ['annual_salary', 'total_debt', 'credit_score'])
    
    for field, name in fields_to_compare:
        current_value = current_data.get(field, 0) or 0
        previous_value = previous_data.get(field, 0) or 0
        
        current_value = int(current_value)
        previous_value = int(previous_value)

        diff = current_value - previous_value
        
        # ğŸš¨ [ìˆ˜ì •]: ì²« ë³´ê³ ì„œê°€ ì•„ë‹ ë•Œë§Œ 0ì´ ì•„ë‹Œ ìœ ì˜ë¯¸í•œ ë³€ë™ì„ ë¹„êµ
        if not is_first_report and diff != 0:
            change_raw_changes.append(f"{name} ë³€ë™: {previous_value:,}ì› â†’ {current_value:,}ì› ({diff:+,}ì›)")
        # ğŸš¨ [ì¶”ê°€]: ì²« ë³´ê³ ì„œì´ê³ , í˜„ì¬ ë°ì´í„°ê°€ 0ì´ ì•„ë‹Œ ê²½ìš° í˜„ì¬ ìƒíƒœë§Œ ê¸°ë¡
        elif is_first_report and current_value != 0:
             change_raw_changes.append(f"ìµœì´ˆ ê¸°ë¡ {name}: {current_value:,}ì›")
    
    if not change_raw_changes:
        return {
            "tool_name": "analyze_user_profile_changes_tool",
            "success": True,
            "change_raw_changes": [],
            "is_first_report": is_first_report,
            "message": "ì§ì „ ë³´ê³ ì„œ ëŒ€ë¹„ ì£¼ìš” ê°œì¸ ê¸ˆìœµ ì§€í‘œ(ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜)ì˜ ë³€ë™ ì‚¬í•­ì€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    return {
        "tool_name": "analyze_user_profile_changes_tool",
        "success": True,
        "change_raw_changes": change_raw_changes,
        "is_first_report": is_first_report
    }