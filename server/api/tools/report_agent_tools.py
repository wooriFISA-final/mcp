import os
import requests
import logging
import pandas as pd
import json
import re 
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, Body
from datetime import datetime, date
from dotenv import load_dotenv, find_dotenv, dotenv_values
from pathlib import Path
from langchain_huggingface import HuggingFaceEndpointEmbeddings 
from langchain_community.vectorstores import FAISS
from pathlib import Path


# ------------------------------------------------------------------
# ğŸ¯ [Environment Cleanup Function] RAG ì—°ê²° ì˜¤ì—¼ ë³€ìˆ˜ ì´ˆê¸°í™”
# ------------------------------------------------------------------
def _cleanup_rag_env():
    """Hugging Face Endpoint ì¶©ëŒì„ ìœ ë°œí•  ìˆ˜ ìˆëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if 'HUGGINGFACE_API_URL' in os.environ:
        del os.environ['HUGGINGFACE_API_URL']
        logger.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HUGGINGFACE_API_URL ê°•ì œ ì œê±°ë¨.")
    if 'HF_ENDPOINT' in os.environ:
        del os.environ['HF_ENDPOINT']
        logger.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HF_ENDPOINT ê°•ì œ ì œê±°ë¨.")

# ğŸ¯ [ENV ë¡œë“œ]
load_dotenv(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv("..")) 

# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • ì „ì— í™˜ê²½ ë³€ìˆ˜ ì •ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
_cleanup_rag_env() 

# ğŸ¯ [ENV íŒŒì¼ ê°’ ì§ì ‘ ë¡œë“œ]: ì…¸ í™˜ê²½ ë³€ìˆ˜ì™€ì˜ ì¶©ëŒì„ ë§‰ê¸° ìœ„í•´ íŒŒì¼ ë‚´ìš©ë§Œ ë‹¤ì‹œ ì½ì–´ì˜µë‹ˆë‹¤.
ENV_VALUES = dotenv_values(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv(".."))


# ğŸ¯ [ìš”ì²­ ê²½ë¡œ ë°˜ì˜]
from server.schemas.report_schema import (
    AnalyzeSpendingInput, AnalyzeSpendingOutput, 
    FinalSummaryInput, FinalSummaryOutput, 
    ToolSkippedOutput, PolicyRAGSearchInput, PolicyRAGSearchOutput 
)

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • (ENV_VALUES ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ) - ì¶©ëŒ ë°©ì§€ ëª©ì 
# ------------------------------------------------------------------
# os.getenv ëŒ€ì‹  ENV_VALUES ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ê°’ì„ ê°€ì ¸ì™€ ì…¸ ì¶©ëŒì„ ë°©ì§€í•©ë‹ˆë‹¤.
OLLAMA_HOST = ENV_VALUES.get("OLLAMA_HOST", 'http://localhost:11434') 
QWEN_MODEL = ENV_VALUES.get("REPORT_LLM", 'qwen3:8b')

# ğŸ›‘ [í•µì‹¬ ì„¤ì •]: HF_EMBEDDING_MODELì„ Qwen ëª¨ë¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
HF_EMBEDDING_MODEL = ENV_VALUES.get("HF_EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
VECTOR_DB_PATH = ENV_VALUES.get("VECTOR_DB_PATH", '../data/faiss_index')
HUGGINGFACEHUB_API_TOKEN = ENV_VALUES.get("HUGGINGFACEHUB_API_TOKEN")

router = APIRouter(
    prefix="/report_processing",
    tags=["Report Processing Tools"] 
)

# ------------------------------------------------------------------
# ğŸ¯ ì •ì±… PDF êµ¬ì¡°ì— ë§ì¶° RAG ê²€ìƒ‰ì„ ìë™í™”í•  í‚¤ì›Œë“œ ëª©ë¡
# ------------------------------------------------------------------
POLICY_SECTIONS_TO_CHECK = [
    "1ì¥ ê°€. ìš©ì–´ì˜ ì •ì˜ ë³€ê²½ ë° ì‹ ì„¤ í•­ëª©",
    "2ì¥ ë‚˜. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ë‹´ë³´ì¸ì •ë¹„ìœ¨(LTV) ë³€ë™ ë° íŠ¹ë¡€ ì ìš©",
    "3ì¥ ë‹¤. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ì´ë¶€ì±„ìƒí™˜ë¹„ìœ¨(DTI) ì ìš© ë° ë°°ì œ ê¸°ì¤€",
    "4ì¥ ë¼. ê³ ì•¡ ê°€ê³„ëŒ€ì¶œ DSR ì ìš© ê¸°ì¤€ ë° ì˜ˆì™¸ ì‚¬í•­",
    "5ì¥ ë§ˆ. ì£¼íƒê´€ë ¨ ë‹´ë³´ëŒ€ì¶œ ì·¨ê¸‰ ê´€ë ¨ ìœ ì˜ì‚¬í•­ ë° íŠ¹ë¡€ ëŒ€ì¶œ ì‹ ì„¤"
]


# ------------------------------------------------------------------
# ğŸ¯ RAG ê²€ìƒ‰ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë‹¨ì¼ íŒŒì¼ í•„í„°ë§ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •)
# ------------------------------------------------------------------
def _rag_similarity_search(query: str, k: int = 5, required_sources: Optional[List[str]] = None) -> str:
    """FAISS DBë¥¼ ë¡œë“œí•˜ì—¬ ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì§€ì •ëœ ì†ŒìŠ¤ íŒŒì¼ ëª©ë¡ì—ì„œë§Œ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""

    if not HUGGINGFACEHUB_API_TOKEN:
        return "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: HUGGINGFACEHUB_API_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
    # ğŸ¯ [ìµœì¢… í™•ì¸]: HF_EMBEDDING_MODEL ë³€ìˆ˜ì˜ í˜„ì¬ ê°’ì„ ì‚¬ìš©
    current_model = HF_EMBEDDING_MODEL 
    logger.info(f"RAG: ì„ë² ë”© ëª¨ë¸ {current_model} ì‚¬ìš©.")


    try:
        # ğŸ¯ HuggingFaceEndpointEmbeddings ì‚¬ìš©
        embeddings = HuggingFaceEndpointEmbeddings(
            model=current_model,
            huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
        )
        
        db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        
        # í•„í„°ë§ì„ ìœ„í•´ ì¶©ë¶„íˆ ë§ì€ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # RAG ê²€ìƒ‰ ê¹Šì´ëŠ” ì •ì±… ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ 7ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
        found_chunks = db.similarity_search(query, k=k * 4) 
        
        context = []
        filtered_count = 0
        
        for chunk in found_chunks:
            source = chunk.metadata.get("source", "ì¶œì²˜ ë¯¸ìƒ")
            
            # ğŸ¯ [í•µì‹¬ ë¡œì§] required_sources ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹ sourceê°€ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            is_valid_source = not required_sources or any(req_src in source for req_src in required_sources)
            
            if not is_valid_source:
                continue

            # í•„í„°ë§ëœ ê²°ê³¼ë§Œ Kê°œê¹Œì§€ ì €ì¥
            if filtered_count < k:
                context.append(f"[ì¶œì²˜: {source}]\n{chunk.page_content}")
                filtered_count += 1
            else:
                break

        if not context:
            source_info = f"ë¬¸ì„œ ëª©ë¡: {required_sources}" if required_sources else "ëª¨ë“  ë¬¸ì„œ"
            return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•´ {source_info}ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
        return "\n---\n".join(context)
    
    except Exception as e:
        logger.error(f"RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}", exc_info=True)
        return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {type(e).__name__} - {e}"


# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜]: ì •ì±… ë¬¸ì„œ ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
# ------------------------------------------------------------------
def _find_latest_policy_file(base_dir: str) -> Optional[str]:
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ 'YYYYMMDD_policy.pdf' íŒ¨í„´ì„ ë”°ë¥´ëŠ” íŒŒì¼ ì¤‘
    ë‚ ì§œê°€ ê°€ì¥ ìµœì‹ ì¸ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # ğŸš¨ [ìˆ˜ì •]: Path ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ë ‰í† ë¦¬ ì ‘ê·¼
    policy_dir = Path(base_dir) 
    
    if not policy_dir.is_dir():
        logger.error(f"ì •ì±… ë¬¸ì„œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}")
        return None
    
    # ì •ê·œí‘œí˜„ì‹: YYYYMMDD_policy.pdf íŒ¨í„´ì— ë§ê³ , ë‚ ì§œ ë¶€ë¶„ì„ ê·¸ë£¹ìœ¼ë¡œ ìº¡ì²˜
    date_file_pattern = re.compile(r'(\d{8})_policy\.pdf$', re.IGNORECASE)
    
    latest_file_info = None # (ë‚ ì§œ, ê²½ë¡œ) íŠœí”Œ ì €ì¥
    
    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ íƒìƒ‰
    for file_path in policy_dir.glob('*_policy.pdf'):
        match = date_file_pattern.search(file_path.name)
        
        if match:
            file_date_str = match.group(1)
            
            # ê°€ì¥ í°(ìµœì‹ ) ë‚ ì§œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            if latest_file_info is None or file_date_str > latest_file_info[0]:
                # ìœˆë„ìš°/ë¦¬ëˆ…ìŠ¤ í™˜ê²½ ëª¨ë‘ì—ì„œ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ strë¡œ ë³€í™˜
                latest_file_info = (file_date_str, str(file_path)) 

    if latest_file_info:
        logger.info(f"RAG: ê°€ì¥ ìµœì‹  ì •ì±… íŒŒì¼ ë°œê²¬: {latest_file_info[1]}")
        return latest_file_info[1]
    else:
        logger.warning(f"RAG: {base_dir}ì—ì„œ ìœ íš¨í•œ ì •ì±… íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ í•¨ìˆ˜]: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ 100% íƒì§€
# ------------------------------------------------------------------
def _find_policies_by_marker_regex(context: str) -> List[Dict[str, str]]:
    """RAG ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ <ì‹ ì„¤ YYYY.M.D.> ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì •ì±… êµ¬ë¬¸ì„ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ ë° ì •ê·œí™”."""
    
    # ğŸš¨ [í•µì‹¬ ìˆ˜ì • 1]: RAG ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶œì²˜(Source) ì •ë³´ì™€ ê´€ë ¨ëœ ëª¨ë“  ë¬¸ìì—´ì„ ë¯¸ë¦¬ ì œê±°í•©ë‹ˆë‹¤.
    context_clean = re.sub(r'\[ì¶œì²˜:.*?\.pdf\]', '', context, flags=re.DOTALL)
    context_clean = re.sub(r'---\n', '', context_clean, flags=re.DOTALL)
    
    # ğŸ¯ [ìˆ˜ì •ëœ ì •ê·œì‹]: ì¡°í•­ ê¸°í˜¸ë¡œ ì‹œì‘í•˜ê³  ë§ˆì»¤ë¡œ ëë‚˜ëŠ” êµ¬ë¬¸ì„ ì •í™•íˆ íƒì§€í•©ë‹ˆë‹¤.
    # [\s\S]*?ëŠ” ê°œí–‰ ë¬¸ìë¥¼ í¬í•¨í•˜ì—¬ ë¹„íƒìš•ì (non-greedy)ìœ¼ë¡œ ë§ˆì»¤ ì§ì „ê¹Œì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¡ìŠµë‹ˆë‹¤.
    regex = r"([\n\s]*([ê°€-í£\d]+\.|\([ê°€-í£\d]+\))[\s\S]*?)\< *(ì‹ ì„¤|ê°œì •)\s*(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*>"
    
    matches = re.findall(regex, context_clean, re.DOTALL) 
    
    extracted_changes = []
    
    for full_text, start_marker, change_type, year, month, day in matches:
        # ì •ì±… ë‚´ìš©: ë§ˆì»¤ ì§ì „ì˜ í…ìŠ¤íŠ¸ì™€ ë§ˆì»¤ë¥¼ í¬í•¨
        policy_text_with_marker = full_text.strip()
        
        # ğŸš¨ [í•µì‹¬ ìˆ˜ì • 2]: ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” í•œê¸€/ì˜ì–´/ìˆ«ì ì‚¬ì´ì— ê³µë°±ì„ ì‚½ì…í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
        normalized_text = re.sub(r'([ê°€-í£a-zA-Z\d])([ê°€-í£a-zA-Z\d])', r'\1 \2', policy_text_with_marker).strip()
        # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        normalized_text = re.sub(r'\s{2,}', ' ', normalized_text)
        
        try:
            effective_date = datetime(int(year), int(month), int(day)).strftime("%Y-%m-%d")
        except ValueError:
            effective_date = "N/A"

        # ë§ˆì»¤ì˜ ë‚ ì§œê°€ 2025-03-05 ì´í›„ì¸ì§€ í™•ì¸ (íŒŒì¼ ë‚ ì§œ ê¸°ì¤€)
        if effective_date >= "2025-03-05": 
            extracted_changes.append({
                "effective_date": effective_date,
                "policy_text": normalized_text
            })

    return extracted_changes


# ------------------------------------------------------------------
# ğŸ¯ [ìˆ˜ì •ëœ ë‚´ë¶€ ë¡œì§ 2]: LLMì„ í†µí•´ ì •ì±… ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¸í’‹)
# ------------------------------------------------------------------
def _generate_final_report_from_structured_data(report_month_str: str, structured_changes: List[Dict[str, str]]) -> Dict[str, Any]:
    """Pythonì´ ì°¾ì€ ì •ì±… ë³€ë™ ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì—ê²Œ ë„˜ê²¨ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    report_month = datetime.strptime(report_month_str, "%Y-%m-%d").date()
    report_month_str_kr = report_month.strftime('%Yë…„ %mì›”')
    
    # LLM ì¸í’‹ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    analysis_input = "\n---\n".join([f"[{c['effective_date']}] {c['policy_text']}" for c in structured_changes])
    
    # ê°€ì¥ ë¹ ë¥¸ ì‹œí–‰ì¼ìë¥¼ ì°¾ì•„ ë¶„ì„ ë³´ê³ ì„œ ì œëª©ì— ì‚¬ìš©
    earliest_date = structured_changes[0]['effective_date'] if structured_changes else "2025-03-05"

    # ğŸ¯ [í”„ë¡¬í”„íŠ¸ ë³€ê²½]: ë‹¨ì¼ ê°„ê²°í•œ ë¶„ì„ ìš”ì•½ë§Œ ìš”ì²­í•˜ë„ë¡ ë³€ê²½
    prompt = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” Pythonì„ í†µí•´ ì¶”ì¶œëœ, 2025ë…„ 3ì›” 5ì¼ ì´í›„ ì‹œí–‰ë  ì •ì±… ë³€ë™ ì‚¬í•­ì˜ í•µì‹¬ ì¡°í•­ ëª©ë¡ì…ë‹ˆë‹¤.
    
    ì´ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ê°ì—ê²Œ ì „ë‹¬í•  **ê°„ê²°í•œ ë‹¨ì¼ ë‹¨ë½ ë¶„ì„ ë³´ê³ ì„œ**ë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    
    **ë³´ê³ ì„œ í˜•ì‹:**
    1. ë°˜ë“œì‹œ 'ğŸ“Œ [ì‹œí–‰ì¼: {earliest_date}]'ë¡œ ì‹œì‘í•˜ì‹­ì‹œì˜¤.
    2. ë³´ê³ ì„œëŠ” í—¤ë”, í‘¸í„°, ì œëª© ì—†ì´ **í•˜ë‚˜ì˜ ê°„ê²°í•œ ë‹¨ë½**ìœ¼ë¡œ êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
    3. ë³€ë™ ì‚¬í•­ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ê³ ê°ì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í¬í•¨í•˜ì—¬ 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤.
    4. **ì •ì±… ë³€ë™ ì‚¬í•­ì˜ ëª©ë¡** ì™¸ì— LTV/DSR ê°™ì€ **ì¼ë°˜ì ì¸ ë°°ê²½ ì •ë³´**ëŠ” í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    
    [ì¶”ì¶œëœ ì •ì±… ë³€ë™ ì‚¬í•­]
    {analysis_input}
    
    [ê°„ê²°í•œ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ]
    """
    
    payload = {"model": QWEN_MODEL, "prompt": prompt, "stream": False, "options": {"temperature": 0.5}}
    
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        final_analysis_report = response.json()['response'].strip()
        
        # ğŸš¨ [Guardrail í•„í„°]: LLMì´ ì¶”ê°€í•œ í—¤ë”, ì„¹ì…˜, êµ¬ë¶„ì ë“±ì„ ê°•ì œë¡œ ì œê±°í•˜ê³  ë‹¨ì¼ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        
        # 1. ëª¨ë“  Markdown Headers ë° êµ¬ë¶„ì ì œê±°
        cleaned_report = re.sub(r'(#+|--+|=+)\s*.*?\n', ' ', final_analysis_report, flags=re.DOTALL)
        # 2. ë‹¤ì¤‘ ê³µë°± ë‹¨ì¼í™” ë° ì•ë’¤ ê³µë°± ì œê±°
        cleaned_report = re.sub(r'\s{2,}', ' ', cleaned_report).strip()
        
        # 3. 'ğŸ“Œ [ì‹œí–‰ì¼: YYYY-MM-DD]' ì ‘ë‘ì‚¬ ê°•ì œ ì ìš© ë° ì¬ì •ë ¬
        earliest_date = structured_changes[0]['effective_date'] if structured_changes else "2025-03-05"
        
        # 'ğŸ“Œ [ì‹œí–‰ì¼: 2025-03-05]' ë¬¸ìì—´ ìì²´ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‚´ìš©ë§Œ ì¶”ì¶œ
        content_only = re.sub(r'^ğŸ“Œ\s*\[ì‹œí–‰ì¼:\s*[\d-]*\s*\]\s*', '', cleaned_report).strip()
        
        final_analysis_report = f"ğŸ“Œ [ì‹œí–‰ì¼: {earliest_date}] {content_only}"
        
        # LLMì´ ë„ˆë¬´ ê¸¸ê²Œ ì¶œë ¥í–ˆë‹¤ë©´ ìë¥´ê±°ë‚˜, ë‹¨ì¼ ë‹¨ë½ìœ¼ë¡œ ê°•ì œ ë³€í™˜
        final_analysis_report = ' '.join(final_analysis_report.split()) # ëª¨ë“  ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ê³  ë‹¨ì¼ ë‹¨ë½ìœ¼ë¡œ ê°•ì œ ë³€í™˜
        
        return {
            "analysis_report": final_analysis_report, 
            "error": None,
        }

    except Exception as e:
        logger.error(f"ìµœì¢… ì •ì±… ë¶„ì„ LLM ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "analysis_report": "ìµœì¢… ì •ì±… ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ", 
            "error": str(e)
        }

# ==============================================================================
# ë…ë¦½ Tool 1: ì†Œë¹„ ë°ì´í„° ë¶„ì„ ë° êµ°ì§‘ ìƒì„± (ë³µêµ¬)
# ==============================================================================
@router.post(
    "/analyze_user_spending",
    summary="ì›”ë³„ ì†Œë¹„ ë°ì´í„° ë¹„êµ ë¶„ì„ ë° êµ°ì§‘ ìƒì„±",
    operation_id="analyze_user_spending_tool", 
    description="ë‘ ë‹¬ì¹˜ ì†Œë¹„ ë°ì´í„°(DataFrame Records)ë¥¼ ë°›ì•„ ì´ ì§€ì¶œ, Top 3 ì¹´í…Œê³ ë¦¬ë¥¼ ë¹„êµ ë¶„ì„í•˜ê³ , êµ°ì§‘ ë³„ëª…ê³¼ ì¡°ì–¸ì„ LLMì„ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_spending(
    consume_records: List[Dict[str, Any]] = Body(..., embed=True),
    member_data: Dict[str, Any] = Body(..., embed=False),
    ollama_model: Optional[str] = Body(QWEN_MODEL, embed=False)
) -> dict:
    """ì†Œë¹„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ°ì§‘ì„ ë¶„ì„í•˜ê³ , LLMì„ í†µí•´ ì¡°ì–¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not consume_records or len(consume_records) < 2:
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": "ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ìµœì†Œ 2ê°œì›” ë°ì´í„° ë¶€ì¡±"}
    
    try:
        df_consume = pd.DataFrame(consume_records)
        df_consume['spend_month'] = pd.to_datetime(df_consume['spend_month'])
        df_consume = df_consume.sort_values(by='spend_month', ascending=False)
        
        feb_data = df_consume.iloc[0] 
        jan_data = df_consume.iloc[1]

        total_spend_feb = feb_data.get('total_spend', 0) or 0
        total_spend_jan = jan_data.get('total_spend', 0) or 0
        diff = total_spend_feb - total_spend_jan
        change_rate = (diff / total_spend_jan) * 100 if total_spend_jan else 0

        cat1_cols = [col for col in feb_data.index if col.startswith('CAT1_')]
        feb_cats = df_consume.iloc[0][cat1_cols].sort_values(ascending=False).head(3) # ìµœì‹  ë°ì´í„° ì‚¬ìš©
        
        # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…: consume_analysis_summaryì— ë§ì¶¤
        consume_analysis_summary = {
            'latest_total_spend': f"{total_spend_feb:,}",
            'total_change_diff': f"{diff:+,}",
            'top_3_categories': [col.replace('CAT1_', '') for col in feb_cats.index],
            'member_info': member_data
        }

        nickname = f"ë ˆì €/ì—¬í–‰ ì§‘ì¤‘í˜• ê³ ê°" # LLMì´ ë³€ê²½í•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ê°’ ì„¤ì •
        prompt = f"""
        [System] ë‹¹ì‹ ì€ ê³ ê°ì˜ ì†Œë¹„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì—ê²Œ ì „ë‹¬í•  4ì¤„ì˜ **ê°„ê²°í•˜ê³  ì •ì¤‘í•œ** ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œì™€ ì €ì¶•/íˆ¬ì ì¡°ì–¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        [ë¶„ì„ ê²°ê³¼]
        ì´ ì§€ì¶œ: {consume_analysis_summary['latest_total_spend']}ì›, ë³€í™”: {consume_analysis_summary['total_change_diff']}ì›. 
        ì£¼ ì†Œë¹„ ì˜ì—­: {', '.join(consume_analysis_summary['top_3_categories'])}. 
        ê³ ê° ì •ë³´: {member_data}
        [ë³´ê³ ì„œ í˜•ì‹]
        1. êµ°ì§‘ ë³„ëª… ì–¸ê¸‰: {nickname}
        2. ì§€ì¶œ ë³€í™” í•´ì„ ë° ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì„¤ëª…
        3. ì—°ë´‰/ë¶€ì±„ ë“±ì„ ê³ ë ¤í•œ ì €ì¶•/íˆ¬ì ì¡°ì–¸ í•œ ì¤„ í¬í•¨ (ì˜ˆ: "ì¦ê°€í•œ ì§€ì¶œì„ ê°ì•ˆí•˜ì—¬..." ë˜ëŠ” "ì•ˆì •ì ì¸ ì—°ë´‰ì„ ë°”íƒ•ìœ¼ë¡œ...")
        """
        
        payload = {"model": QWEN_MODEL, "prompt": prompt, "stream": False}
        
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        consume_report = response.json()['response'].strip()
        
        # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…: consume_report, consume_analysis_summary
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": True, 
            "consume_report": consume_report,
            "cluster_nickname": nickname,
            "consume_analysis_summary": consume_analysis_summary
        }

    except Exception as e:
        logger.error(f"ì†Œë¹„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": str(e)}

# ==============================================================================
# ë…ë¦½ Tool 2: ìµœì¢… 3ì¤„ ìš”ì•½ LLM Tool (ë³µêµ¬)
# ==============================================================================
@router.post(
    "/generate_final_summary",
    summary="ìµœì¢… ë³´ê³ ì„œ 3ì¤„ ìš”ì•½ ìƒì„±",
    operation_id="generate_final_summary_llm", 
    description="í†µí•© ë³´ê³ ì„œ ë³¸ë¬¸ì„ ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_generate_final_summary(report_content: str = Body(..., embed=True)) -> dict:
    """Agentê°€ ë³´ê³ ì„œ ë³¸ë¬¸ì„ ì „ì†¡í•˜ë©´, LLMì„ í†µí•´ 3ì¤„ í•µì‹¬ ìš”ì•½ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ğŸ¯ [ìˆ˜ì •] êµ¬ë¶„ì ë¬´ì‹œ ì§€ì¹¨ í¬í•¨
    prompt_template = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ í†µí•© ë³´ê³ ì„œ ë‚´ìš©ì„ ì½ê³ , **ê°€ì¥ í•µì‹¬ì ì¸ 3ê°€ì§€ ì‚¬í•­**ë§Œ ë½‘ì•„ ê°„ê²°í•˜ê²Œ **3ì¤„**ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤. ë³´ê³ ì„œ ë³¸ë¬¸ ì™¸ì˜ ì„¤ëª…ì´ë‚˜ ì œëª©, ë˜ëŠ” êµ¬ë¶„ì(---SECTION_END---)ì™€ ê°™ì€ **ë¶ˆí•„ìš”í•œ ê¸°í˜¸ëŠ” ëª¨ë‘ ë¬´ì‹œ**í•˜ì‹­ì‹œì˜¤.
    
    [í†µí•© ë³´ê³ ì„œ ë‚´ìš©]
    {report_content}
    
    [3ì¤„ ìš”ì•½]
    """
    
    payload = {"model": QWEN_MODEL, "prompt": prompt_template, "stream": False, "options": {"temperature": 0.3}}
    
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        final_summary = response.json()['response'].strip()
        lines = [line.strip() for line in final_summary.split('\n') if line.strip()]
        threelines_summary = "\n".join(lines[:3]) # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…ì— ë§ì¶¤
        
        return {"tool_name": "generate_final_summary_llm", "success": True, "threelines_summary": threelines_summary}
    except requests.exceptions.RequestException as e:
        error_msg = f"Ollama í†µì‹  ì˜¤ë¥˜: {e}"
        return {"tool_name": "generate_final_summary_llm", "success": False, "error": error_msg, "threelines_summary": "3ì¤„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"}


# # ------------------------------------------------------------------
# ğŸ¯ íˆ´ 3-D: ì •ì±… ë³€ë™ ìë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„± íˆ´ (í•µì‹¬)
# ------------------------------------------------------------------
@router.post(
    "/check_and_report_policy_changes",
    summary="ë§¤ì›” ìë™ ì •ì±… ë³€ë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="check_and_report_policy_changes_tool", 
    description="ì‚¬ìš©ì ì…ë ¥ ì—†ì´, ì •ì˜ëœ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ë³€ë™ ì‚¬í•­ì„ í™•ì¸í•˜ê³  êµ¬ì¡°í™”ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_check_policy_changes(
    report_month_str: str = Body(..., embed=True) 
) -> dict:
    """ë§¤ì›” ì •ê¸° ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì •ì±… ë³€ë™ ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ë¹„êµí•˜ê³  ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ğŸ¯ [ìˆ˜ì • 1] RAG ê²€ìƒ‰ ëŒ€ìƒì„ ë™ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    POLICY_DOCUMENT_DIR = '../data/policy_documents' # ê²½ë¡œ ìˆ˜ì • ë°˜ì˜
    LATEST_POLICY_SOURCE = _find_latest_policy_file(POLICY_DOCUMENT_DIR) # _find_latest_policy_file í•¨ìˆ˜ëŠ” ì™¸ë¶€ ì •ì˜ë¨
    
    if not LATEST_POLICY_SOURCE:
        analysis_report = "ì •ì±… ë¬¸ì„œ ë””ë ‰í† ë¦¬ì—ì„œ ìœ íš¨í•œ ìµœì‹  ì •ì±… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": False, 
            "analysis_report": analysis_report, 
            "policy_changes": [],
            "error": "ìµœì‹  ì •ì±… íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨",
        }
        
    REQUIRED_SOURCES = [LATEST_POLICY_SOURCE] 
    
    # 1. ğŸ“… ë‚ ì§œ ì²´í¬ ë° ì´ˆê¸° ì„¤ì • (ë³´ê³ ì„œ ìœ íš¨ì„± ì²´í¬ ë° ë‹¨ì¼ ë³´ê³  ì£¼ê¸° ì²´í¬)
    try:
        report_month = datetime.strptime(report_month_str, "%Y-%m-%d").date()
        
        # ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ
        file_name = Path(LATEST_POLICY_SOURCE).name 
        latest_policy_date_str = file_name.split('_')[0] # 'YYYYMMDD' ì¶”ì¶œ
        latest_policy_date = datetime.strptime(latest_policy_date_str, "%Y%m%d").date()
        
        # ğŸ¯ [ìˆ˜ì • 2] ìµœì†Œ í•„í„° ë‚ ì§œë¥¼ ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        MINIMUM_FILTER_DATE_DT = latest_policy_date 
        
        # ----------------------------------------------------------------------
        # ğŸ¯ [í•µì‹¬ ì¶”ê°€] ë‹¨ì¼ ë³´ê³  ì£¼ê¸° í™•ì¸ ë¡œì§: ì •ì±… ë³€ë™ì€ ë‹¤ìŒ ë‹¬ ë³´ê³ ì„œì—ë§Œ ë°˜ì˜
        # ----------------------------------------------------------------------
        
        # 1. ìµœì‹  ì •ì±… ë¬¸ì„œ ë‚ ì§œ ì´í›„ì˜ 'ë‹¤ìŒ ë‹¬ 1ì¼'ì„ ê³„ì‚°í•©ë‹ˆë‹¤. (Target Report Month)
        policy_year = latest_policy_date.year
        policy_month = latest_policy_date.month
        
        # ë‹¤ìŒ ë‹¬ ê³„ì‚° (12ì›” -> ë‹¤ìŒ í•´ 1ì›”ë¡œ ì •í™•íˆ ë„˜ì–´ê°)
        next_month = (policy_month % 12) + 1
        next_year = policy_year + (1 if policy_month == 12 else 0)
        target_report_month_start = date(next_year, next_month, 1)
        
        # 2. í˜„ì¬ ìš”ì²­ëœ ë³´ê³ ì„œ ì›”ì˜ ì‹œì‘ì¼ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        report_month_start = report_month.replace(day=1)
        
        # 3. Target Monthê°€ ì•„ë‹ˆë¼ë©´, ë³€ë™ ì—†ìŒ ì²˜ë¦¬ (ì´ë¯¸ ë³´ê³ ë˜ì—ˆê±°ë‚˜, ì•„ì§ ì •ì±… ì‹œí–‰ ì „)
        if report_month_start != target_report_month_start:
            
            # A. ë³´ê³ ì„œ ì›”ì´ ì •ì±… ì‹œí–‰ì¼ë³´ë‹¤ ì´ì „ì¸ ê²½ìš° (ì•„ì§ ì •ì±… ì‹œí–‰ ì „ì´ê±°ë‚˜ íŒŒì¼ ë‚ ì§œ ì´ì „)
            if report_month < latest_policy_date:
                policy_analysis_report = f"{report_month.strftime('%Yë…„ %mì›”')} ë³´ê³ ì„œ ê¸°ì¤€, **ìµœì‹  ì •ì±… ë¬¸ì„œ({latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')})**ê°€ ì•„ì§ ìœ íš¨í•˜ì§€ ì•Šì•„ ì •ì±… ë³€ë™ ì‚¬í•­ì€ ì—†ìŠµë‹ˆë‹¤."
            
            # B. ë³´ê³ ì„œ ì›”ì´ ì •ì±… ì‹œí–‰ ì›”ë³´ë‹¤ ì´í›„ì¸ ê²½ìš° (ì´ë¯¸ ì§€ë‚œë‹¬ì— ë³´ê³  ì™„ë£Œ)
            else: 
                policy_analysis_report = f"ìµœì‹  ì •ì±… ë¬¸ì„œ({latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')})ì˜ ë³€ë™ ì‚¬í•­ì€ ì´ë¯¸ {target_report_month_start.strftime('%Yë…„ %mì›”')} ë³´ê³ ì„œì— ë°˜ì˜ë˜ì—ˆìœ¼ë©°, í˜„ì¬({report_month_start.strftime('%Yë…„ %mì›”')}) ê¸°ì¤€ìœ¼ë¡œ ìƒˆë¡œìš´ ì •ì±… ë³€ë™ ì‚¬í•­ì€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "tool_name": "check_and_report_policy_changes_tool", 
                "success": True, 
                "analysis_report": policy_analysis_report,
                "policy_changes": [],
                "error": None,
            }
        
    except ValueError:
        analysis_report = "ìœ íš¨í•˜ì§€ ì•Šì€ ë³´ê³ ì„œ ì›” í˜•ì‹ì…ë‹ˆë‹¤."
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": False, 
            "analysis_report": analysis_report, # KEY CHANGE
            "policy_changes": [],
            "error": "ë³´ê³ ì„œ ì›” í˜•ì‹ ì˜¤ë¥˜",
        }
    
    # ----------------------------------------------------------------------
    # 4. [í†µê³¼] ë³´ê³ ì„œ ì›”ì´ Target Monthì™€ ì¼ì¹˜í•˜ë¯€ë¡œ, ë³€ë™ì‚¬í•­ ì¶”ì¶œ ì‹œì‘
    # ----------------------------------------------------------------------
    
    # 2. âš™ï¸ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ ì‹¤í–‰ (ìµœì‹  íŒŒì¼ë§Œ ëŒ€ìƒ)
    full_context_list = []
    
    # ğŸ¯ [ìˆ˜ì •] ì •ì±… ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ RAG ê²€ìƒ‰ ê¹Šì´ K_SEARCHë¥¼ 7ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    K_SEARCH = 7 
    
    for section_query in POLICY_SECTIONS_TO_CHECK:
        rag_context = _rag_similarity_search(
            query=section_query, 
            k=K_SEARCH, 
            required_sources=REQUIRED_SOURCES 
        )

        if "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨" not in rag_context:
            full_context_list.append(rag_context)
        else:
            # RAG ê²€ìƒ‰ ìì²´ì˜ ì‹œìŠ¤í…œ ì˜¤ë¥˜ëŠ” 500ì´ ì•„ë‹Œ íˆ´ ì—ëŸ¬ë¡œ ì²˜ë¦¬
             return {
                "tool_name": "check_and_report_policy_changes_tool", 
                "success": False, 
                "analysis_report": "ì •ì±… ë³€ë™ ë¶„ì„ ì‹œìŠ¤í…œ ì˜¤ë¥˜: RAG ê²€ìƒ‰ ì‹¤íŒ¨", # KEY CHANGE
                "policy_changes": [],
                "error": rag_context,
            }

    combined_context = "\n---\n".join(full_context_list)
    
    # 3. ğŸ“ [í•µì‹¬ ë³€ê²½] ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ RAG ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ ì¶”ì¶œ
    structured_changes_raw = _find_policies_by_marker_regex(combined_context)
    
    # 4. ğŸ¯ [ìµœì¢… íŒŒì´ì¬ í•„í„°ë§] ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œ ì´ì „ í•­ëª© ì œê±°
    filtered_changes = []
    
    for change in structured_changes_raw:
        effective_date_str = change.get("effective_date")
        if not effective_date_str or effective_date_str == "N/A":
            continue
            
        try:
            effective_date = datetime.strptime(effective_date_str, "%Y-%m-%d").date()
            
            # [ìµœì¢… í•„í„°ë§] ì‹œí–‰ì¼ì´ MINIMUM_FILTER_DATE_DT (ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œ)ì™€ ê°™ê±°ë‚˜ ì´í›„ì¸ ê²½ìš°ë§Œ í¬í•¨
            if effective_date >= MINIMUM_FILTER_DATE_DT:
                filtered_changes.append(change)
                
        except ValueError:
            # ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí•œ í•­ëª©ì€ ì œì™¸
            continue
            
    structured_changes = filtered_changes

    # 5. ğŸ§© ì¶”ì¶œ ê²°ê³¼ ì²˜ë¦¬ (ë³€ë™ ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš°ì—ë„ Target Monthë¼ë©´, ì •ì±… íŒŒì¼ ìì²´ì— ë³€ë™ ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš°)
    if not structured_changes:
        # ì •ì±… íŒŒì¼ì€ ìµœì‹ ì¸ë°, ê·¸ ì•ˆì— ë§ˆì»¤ë¡œ í‘œì‹œëœ ì‹ ì„¤/ê°œì • ì‚¬í•­ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
        policy_analysis_report = f"{report_month.strftime('%Yë…„ %mì›”')} ë³´ê³ ì„œ ê¸°ì¤€, ìµœì‹  ì •ì±… ë¬¸ì„œ({latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')})ì— **ì‹ ì„¤ ë˜ëŠ” ê°œì •ëœ ì •ì±… ë³€ë™ ì‚¬í•­ì€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**"
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": True, 
            "analysis_report": policy_analysis_report, # KEY CHANGE
            "policy_changes": [],
            "error": None,
        }

    # 6. ğŸ“ LLMì—ê²Œ ë¶„ì„ ìš”ì²­ ë° ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    report_result = _generate_final_report_from_structured_data(report_month_str, structured_changes)
    
    final_analysis_report = report_result['analysis_report']
    
    # 7. ğŸ¯ ìµœì¢… ì•„ì›ƒí’‹ êµ¬ì„±
    return {
        "tool_name": "check_and_report_policy_changes_tool", 
        "success": report_result['error'] is None, 
        "analysis_report": final_analysis_report, # KEY CHANGE
        "policy_changes": structured_changes, # Pythonì´ ì°¾ì€ ì •í™•í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        "error": report_result['error']
    }


# ==============================================================================
# ë…ë¦½ Tool 1: ì†Œë¹„ ë°ì´í„° ë¶„ì„ ë° êµ°ì§‘ ìƒì„± (ë³µêµ¬)
# ==============================================================================
@router.post(
    "/analyze_user_spending",
    summary="ì›”ë³„ ì†Œë¹„ ë°ì´í„° ë¹„êµ ë¶„ì„ ë° êµ°ì§‘ ìƒì„±",
    operation_id="analyze_user_spending_tool", 
    description="ë‘ ë‹¬ì¹˜ ì†Œë¹„ ë°ì´í„°(DataFrame Records)ë¥¼ ë°›ì•„ ì´ ì§€ì¶œ, Top 3 ì¹´í…Œê³ ë¦¬ë¥¼ ë¹„êµ ë¶„ì„í•˜ê³ , êµ°ì§‘ ë³„ëª…ê³¼ ì¡°ì–¸ì„ LLMì„ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_spending(
    consume_records: List[Dict[str, Any]] = Body(..., embed=True),
    member_data: Dict[str, Any] = Body(..., embed=False),
    ollama_model: Optional[str] = Body(QWEN_MODEL, embed=False)
) -> dict:
    """ì†Œë¹„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ°ì§‘ì„ ë¶„ì„í•˜ê³ , LLMì„ í†µí•´ ì¡°ì–¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not consume_records or len(consume_records) < 2:
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": "ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ìµœì†Œ 2ê°œì›” ë°ì´í„° ë¶€ì¡±"}
    
    try:
        df_consume = pd.DataFrame(consume_records)
        df_consume['spend_month'] = pd.to_datetime(df_consume['spend_month'])
        df_consume = df_consume.sort_values(by='spend_month', ascending=False)
        
        feb_data = df_consume.iloc[0] 
        jan_data = df_consume.iloc[1]

        total_spend_feb = feb_data.get('total_spend', 0) or 0
        total_spend_jan = jan_data.get('total_spend', 0) or 0
        diff = total_spend_feb - total_spend_jan
        change_rate = (diff / total_spend_jan) * 100 if total_spend_jan else 0

        cat1_cols = [col for col in feb_data.index if col.startswith('CAT1_')]
        feb_cats = df_consume.iloc[0][cat1_cols].sort_values(ascending=False).head(3) # ìµœì‹  ë°ì´í„° ì‚¬ìš©
        
        # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…: consume_analysis_summaryì— ë§ì¶¤
        consume_analysis_summary = {
            'latest_total_spend': f"{total_spend_feb:,}",
            'total_change_diff': f"{diff:+,}",
            'top_3_categories': [col.replace('CAT1_', '') for col in feb_cats.index],
            'member_info': member_data
        }

        nickname = f"ë ˆì €/ì—¬í–‰ ì§‘ì¤‘í˜• ê³ ê°" # LLMì´ ë³€ê²½í•  ìˆ˜ ìˆì§€ë§Œ, ê¸°ë³¸ê°’ ì„¤ì •
        prompt = f"""
        [System] ë‹¹ì‹ ì€ ê³ ê°ì˜ ì†Œë¹„ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì—ê²Œ ì „ë‹¬í•  4ì¤„ì˜ **ê°„ê²°í•˜ê³  ì •ì¤‘í•œ** ì†Œë¹„ ë¶„ì„ ë³´ê³ ì„œì™€ ì €ì¶•/íˆ¬ì ì¡°ì–¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        [ë¶„ì„ ê²°ê³¼]
        ì´ ì§€ì¶œ: {consume_analysis_summary['latest_total_spend']}ì›, ë³€í™”: {consume_analysis_summary['total_change_diff']}ì›. 
        ì£¼ ì†Œë¹„ ì˜ì—­: {', '.join(consume_analysis_summary['top_3_categories'])}. 
        ê³ ê° ì •ë³´: {member_data}
        [ë³´ê³ ì„œ í˜•ì‹]
        1. êµ°ì§‘ ë³„ëª… ì–¸ê¸‰: {nickname}
        2. ì§€ì¶œ ë³€í™” í•´ì„ ë° ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì„¤ëª…
        3. ì—°ë´‰/ë¶€ì±„ ë“±ì„ ê³ ë ¤í•œ ì €ì¶•/íˆ¬ì ì¡°ì–¸ í•œ ì¤„ í¬í•¨ (ì˜ˆ: "ì¦ê°€í•œ ì§€ì¶œì„ ê°ì•ˆí•˜ì—¬..." ë˜ëŠ” "ì•ˆì •ì ì¸ ì—°ë´‰ì„ ë°”íƒ•ìœ¼ë¡œ...")
        """
        
        payload = {"model": QWEN_MODEL, "prompt": prompt, "stream": False}
        
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        consume_report = response.json()['response'].strip()
        
        # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…: consume_report, consume_analysis_summary
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": True, 
            "consume_report": consume_report,
            "cluster_nickname": nickname,
            "consume_analysis_summary": consume_analysis_summary
        }

    except Exception as e:
        logger.error(f"ì†Œë¹„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": str(e)}

# ==============================================================================
# ë…ë¦½ Tool 2: ìµœì¢… 3ì¤„ ìš”ì•½ LLM Tool (ë³µêµ¬)
# ==============================================================================
@router.post(
    "/generate_final_summary",
    summary="ìµœì¢… ë³´ê³ ì„œ 3ì¤„ ìš”ì•½ ìƒì„±",
    operation_id="generate_final_summary_llm", 
    description="í†µí•© ë³´ê³ ì„œ ë³¸ë¬¸ì„ ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_generate_final_summary(report_content: str = Body(..., embed=True)) -> dict:
    """Agentê°€ ë³´ê³ ì„œ ë³¸ë¬¸ì„ ì „ì†¡í•˜ë©´, LLMì„ í†µí•´ 3ì¤„ í•µì‹¬ ìš”ì•½ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ğŸ¯ [ìˆ˜ì •] êµ¬ë¶„ì ë¬´ì‹œ ì§€ì¹¨ í¬í•¨
    prompt_template = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ í†µí•© ë³´ê³ ì„œ ë‚´ìš©ì„ ì½ê³ , **ê°€ì¥ í•µì‹¬ì ì¸ 3ê°€ì§€ ì‚¬í•­**ë§Œ ë½‘ì•„ ê°„ê²°í•˜ê²Œ **3ì¤„**ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤. ë³´ê³ ì„œ ë³¸ë¬¸ ì™¸ì˜ ì„¤ëª…ì´ë‚˜ ì œëª©, ë˜ëŠ” êµ¬ë¶„ì(---SECTION_END---)ì™€ ê°™ì€ **ë¶ˆí•„ìš”í•œ ê¸°í˜¸ëŠ” ëª¨ë‘ ë¬´ì‹œ**í•˜ì‹­ì‹œì˜¤.
    
    [í†µí•© ë³´ê³ ì„œ ë‚´ìš©]
    {report_content}
    
    [3ì¤„ ìš”ì•½]
    """
    
    payload = {"model": QWEN_MODEL, "prompt": prompt_template, "stream": False, "options": {"temperature": 0.3}}
    
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        final_summary = response.json()['response'].strip()
        lines = [line.strip() for line in final_summary.split('\n') if line.strip()]
        threelines_summary = "\n".join(lines[:3]) # ğŸ¯ [ìˆ˜ì •] ì•„ì›ƒí’‹ í•„ë“œëª…ì— ë§ì¶¤
        
        return {"tool_name": "generate_final_summary_llm", "success": True, "threelines_summary": threelines_summary}
    except requests.exceptions.RequestException as e:
        error_msg = f"Ollama í†µì‹  ì˜¤ë¥˜: {e}"
        return {"tool_name": "generate_final_summary_llm", "success": False, "error": error_msg, "threelines_summary": "3ì¤„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"}


# ==============================================================================
# ë…ë¦½ Tool 4: ì†ìµ/ì§„ì²™ë„ ë¶„ì„ (ì™„ì„±)
# ==============================================================================
@router.post(
    "/analyze_investment_profit",
    summary="íˆ¬ì ìƒí’ˆ ì†ìµ/ì§„ì²™ë„ ë¶„ì„",
    operation_id="analyze_investment_profit_tool", 
    description="ì˜ˆê¸ˆ, ì ê¸ˆ, í€ë“œì˜ ìˆ˜ìµë¥ ê³¼ ì§„ì²™ë„ë¥¼ ë¶„ì„í•˜ê³  LLMì„ í†µí•´ ì¡°ì–¸ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_analyze_investment_profit(products: List[Dict[str, Any]] = Body(..., embed=True)) -> dict:
    """ë³´ìœ  ìƒí’ˆ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì†ìµ ë¶„ì„ ë° ì¡°ì–¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    if not products:
        return {
            "tool_name": "analyze_investment_profit_tool", 
            "success": True, 
            "error": None,
            "profit_analysis_report": "í˜„ì¬ ë³´ìœ  ì¤‘ì¸ íˆ¬ì ìƒí’ˆì´ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆí‚µë‹ˆë‹¤."
        }

    # 1. ğŸ“Š ìƒí’ˆ ë°ì´í„° ì²˜ë¦¬ (ë¶„ì„)
    total_principal = 0
    total_valuation = 0
    
    # ğŸ¯ ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë”°ë¼ total_principal, current_valuation í•„ë“œë¥¼ ê°€ì •
    for p in products:
        principal = p.get('total_principal', 0)
        valuation = p.get('current_valuation', 0)
        total_principal += principal
        total_valuation += valuation 

    net_profit = total_valuation - total_principal
    profit_rate = (net_profit / total_principal) * 100 if total_principal else 0
    
    # 2. ğŸ’¬ [LLM] ë¶„ì„ ìš”ì²­ (ì§„ì²™ë„ ë° ì¡°ì–¸ ìƒì„±)
    data_summary = f"""
    [íˆ¬ì ë¶„ì„ ìš”ì•½]
    - ì´ íˆ¬ì ì›ê¸ˆ: {total_principal:,}ì›
    - í˜„ì¬ í‰ê°€ì•¡: {total_valuation:,}ì›
    - ìˆœì†ìµ: {net_profit:+,}ì›
    - ìˆ˜ìµë¥ : {profit_rate:.2f}%
    - ë³´ìœ  ìƒí’ˆ ìˆ˜: {len(products)}ê°œ
    """
    
    prompt = f"""
    [System] ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ì¡°ì–¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ íˆ¬ì ë¶„ì„ ìš”ì•½ì„ ë³´ê³ , ê³ ê°ì—ê²Œ í˜„ì¬ì˜ íˆ¬ì ì§„ì²™ë„(ìˆ˜ìµë¥ )ì— ëŒ€í•´ í‰ê°€í•˜ê³  **ë‹¤ìŒ ë‹¨ê³„ì˜ íˆ¬ì ì „ëµ**ì— ëŒ€í•œ ì¡°ì–¸ì„ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ì •ì¤‘í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. (ì˜ˆ: "ì•ˆì •ì ì¸ ìˆ˜ìµë¥ ì´ì§€ë§Œ, ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë ¤ë©´ ë¶„ì‚° íˆ¬ìë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    {data_summary}
    
    [íˆ¬ì ì§„ì²™ë„ í‰ê°€ ë° ì¡°ì–¸]
    """
    
    payload = {"model": QWEN_MODEL, "prompt": prompt, "stream": False, "options": {"temperature": 0.5}}
    
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
        profit_analysis_report = response.json()['response'].strip()
        
        return {
            "tool_name": "analyze_investment_profit_tool", 
            "success": True, 
            "profit_analysis_report": profit_analysis_report,
            "net_profit": net_profit,
            "profit_rate": profit_rate,
            "error": None
        }

    except Exception as e:
        logger.error(f"íˆ¬ì ìƒí’ˆ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "tool_name": "analyze_investment_profit_tool", 
            "success": False, 
            "error": f"íˆ¬ì ìƒí’ˆ ë¶„ì„ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}"
        }

# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ] Tool 5: ì‚¬ìš©ì í”„ë¡œí•„ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± (ì¶”ê°€ë¨)
# ------------------------------------------------------------------
@router.post(
    "/analyze_user_profile_changes",
    summary="ì‚¬ìš©ì ê°œì¸ ì§€ìˆ˜ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="analyze_user_profile_changes_tool",
    description="ì§ì „ ë³´ê³ ì„œì™€ í˜„ì¬ DBì—ì„œ ì¡°íšŒí•œ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ë¥¼ ë¹„êµí•˜ê³ , LLMì„ í†µí•´ ë³€ë™ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_profile_changes(
    current_data: Dict[str, Any] = Body(..., embed=True),
    previous_data: Dict[str, Any] = Body(..., embed=False)
) -> dict:
    """ì‚¬ìš©ìì˜ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ ë³€ë™ ì‚¬í•­ì„ ë¶„ì„í•˜ê³  ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # 1. ğŸ“Š ë°ì´í„° ë¹„êµ ë° ìš”ì•½
    change_raw_changes = []
    
    # [ì—°ë´‰ ë¹„êµ]
    current_salary = current_data.get('annual_salary', 0) or 0
    previous_salary = previous_data.get('annual_salary', 0) or 0
    salary_diff = current_salary - previous_salary
    if salary_diff != 0:
        change_raw_changes.append(f"ì—°ë´‰ ë³€ë™: {previous_salary:,}ì› â†’ {current_salary:,}ì› ({salary_diff:+,}ì›)")
    
    # [ë¶€ì±„ ë¹„êµ]
    current_debt = current_data.get('total_debt', 0) or 0
    previous_debt = previous_data.get('total_debt', 0) or 0
    debt_diff = current_debt - previous_debt
    if debt_diff != 0:
        change_raw_changes.append(f"ì´ ë¶€ì±„ ë³€ë™: {previous_debt:,}ì› â†’ {current_debt:,}ì› ({debt_diff:+,}ì›)")

    # [ì‹ ìš© ì ìˆ˜ ë¹„êµ]
    current_credit = current_data.get('credit_score', 0) or 0
    previous_credit = previous_data.get('credit_score', 0) or 0
    credit_diff = current_credit - previous_credit
    if credit_diff != 0:
        change_raw_changes.append(f"ì‹ ìš© ì ìˆ˜ ë³€ë™: {previous_credit}ì  â†’ {current_credit}ì  ({credit_diff:+,}ì )")
    
    analysis_summary = "\n".join(change_raw_changes) if change_raw_changes else "ì§ì „ ë³´ê³ ì„œ ëŒ€ë¹„ ì£¼ìš” ê°œì¸ ê¸ˆìœµ ì§€í‘œ(ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜)ì˜ ë³€ë™ ì‚¬í•­ì€ ì—†ìŠµë‹ˆë‹¤."
    
    # 2. ğŸ’¬ LLM í”„ë¡¬í”„íŠ¸ ìƒì„± (ë³€ë™ ì‚¬í•­ ë¶„ì„ ìš”ì²­)
    if not change_raw_changes:
        change_analysis_report = "ì§ì „ ë³´ê³ ì„œ ëŒ€ë¹„ ê³ ê°ë‹˜ì˜ ì£¼ìš” ê°œì¸ ì§€í‘œ(ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜)ì— í° ë³€ë™ ì‚¬í•­ì´ ì—†ì–´ íŠ¹ì´ ë³´ê³ ëŠ” ìƒëµí•©ë‹ˆë‹¤."
        success = True
    else:
        prompt = f"""
        [System] ë‹¹ì‹ ì€ ê³ ê°ì˜ ê°œì¸ ê¸ˆìœµ ì§€í‘œ(ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜) ë³€ë™ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¹„êµ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì—ê²Œ ì „ë‹¬í•  4ì¤„ì˜ **ê°„ê²°í•˜ê³  ì •ì¤‘í•œ** ë³€ë™ ë¶„ì„ ë³´ê³ ì„œì™€ ê°œì¸ ì¬ì • ìƒí™©ì— ë§ëŠ” ì¡°ì–¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
        
        [ì§€í‘œ ë³€ë™ ê²°ê³¼]
        {analysis_summary}
        
        [ë³´ê³ ì„œ í˜•ì‹]
        1. ì‹ ìš© ì ìˆ˜ ë³€í™”ë¥¼ í¬í•¨í•˜ì—¬ ì§€í‘œ ë³€ë™ì˜ í•µì‹¬ ìš”ì•½
        2. ë¶€ì±„/ì—°ë´‰ ë³€í™”ì— ë”°ë¥¸ ì¬ì • ê±´ì „ì„± í‰ê°€
        3. ë³€ë™ëœ ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê³ ë ¤í•´ì•¼ í•  ì¬ì • ì¡°ì–¸
        """
        
        payload = {"model": QWEN_MODEL, "prompt": prompt, "stream": False, "options": {"temperature": 0.5}}
        
        try:
            response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=180) 
            change_analysis_report = response.json()['response'].strip()
            success = True
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ë³€ë™ ë¶„ì„ LLM ì˜¤ë¥˜: {e}")
            change_analysis_report = "ì‚¬ìš©ì ë³€ë™ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            success = False

    # ğŸ¯ [ìˆ˜ì • ì™„ë£Œ] ì•„ì›ƒí’‹ í•„ë“œëª…ì„ ìš”ì²­í•˜ì‹ ëŒ€ë¡œ ë³€ê²½
    return {
        "tool_name": "analyze_user_profile_changes_tool", 
        "success": success, 
        "change_analysis_report": change_analysis_report,
        "change_raw_changes": change_raw_changes
    }