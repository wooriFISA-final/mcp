import os
import logging
import pandas as pd
import json
import re 
import time 
import glob  
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, Body
from datetime import datetime, date
from dateutil.relativedelta import relativedelta 
from dotenv import load_dotenv, find_dotenv, dotenv_values
from pathlib import Path
from langchain_huggingface import HuggingFaceEndpointEmbeddings 
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sqlalchemy import create_engine, text
from decimal import Decimal


# ------------------------------------------------------------------
# ğŸ¯ [Environment Cleanup Function] RAG ì—°ê²° ì˜¤ì—¼ ë³€ìˆ˜ ì´ˆê¸°í™”
# ------------------------------------------------------------------
def _cleanup_rag_env():
    """Hugging Face Endpoint ì¶©ëŒì„ ìœ ë°œí•  ìˆ˜ ìˆëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if 'HUGGINGFACE_API_URL' in os.environ:
        del os.environ['HUGGINGFACE_API_URL']
        logging.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HUGGINGFACE_API_URL ê°•ì œ ì œê±°ë¨.")
    if 'HF_ENDPOINT' in os.environ:
        del os.environ['HF_ENDPOINT']
        logging.warning("RAG: í™˜ê²½ ë³€ìˆ˜ HF_ENDPOINT ê°•ì œ ì œê±°ë¨.")

# ğŸ¯ [ENV ë¡œë“œ]
load_dotenv(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv("..")) 

# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • ì „ì— í™˜ê²½ ë³€ìˆ˜ ì •ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
_cleanup_rag_env() 

# ğŸ¯ [ENV íŒŒì¼ ê°’ ì§ì ‘ ë¡œë“œ]: ì…¸ í™˜ê²½ ë³€ìˆ˜ì™€ì˜ ì¶©ëŒì„ ë§‰ê¸° ìœ„í•´ íŒŒì¼ ë‚´ìš©ë§Œ ë‹¤ì‹œ ì½ì–´ì˜µë‹ˆë‹¤.
ENV_VALUES = dotenv_values(find_dotenv(usecwd=True, raise_error_if_not_found=False) or find_dotenv(usecwd=True) or find_dotenv(".."))

# ğŸ¯ [ìš”ì²­ ê²½ë¡œ ë°˜ì˜]
from server.schemas.report_schema import (
    AnalyzeSpendingInput, AnalyzeSpendingOutput, 
    FinalSummaryInput, FinalSummaryOutput, 
    ToolSkippedOutput, PolicyRAGSearchInput, PolicyRAGSearchOutput 
)


logger = logging.getLogger(__name__)

# ------------------------------------------------------------------
# ğŸ¯ [ENV ë³€ìˆ˜] ì„¤ì • (ENV_VALUES ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ) - ì¶©ëŒ ë°©ì§€ ëª©ì 
# ------------------------------------------------------------------
# RAG ë° ì •ì±… ê²€ìƒ‰ì— í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
HF_EMBEDDING_MODEL = ENV_VALUES.get("HF_EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
VECTOR_DB_PATH = ENV_VALUES.get("VECTOR_DB_PATH", './data/faiss_index')
HUGGINGFACEHUB_API_TOKEN = ENV_VALUES.get("HUGGINGFACEHUB_API_TOKEN")



# ğŸš¨ [ì¶”ê°€] ì •ì±… ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
POLICY_DIR = "./data/policy_documents"


router = APIRouter(
    prefix="/report_processing",
    tags=["Report Processing Tools"] 
    
)

# ------------------------------------------------------------------
# ğŸ¯ [DB ì—°ê²° ì„¤ì •] Agent Toolsì—ì„œ ì§ì ‘ DB ì¡°íšŒ
# ------------------------------------------------------------------
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
DB_NAME = os.getenv("DB_NAME")

engine = None
if DB_USER and DB_PASSWORD and DB_HOST and DB_NAME:
    try:
        engine = create_engine(f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}")
        logger.info("âœ… Report Agent Tools DB Engine ìƒì„± ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ DB Engine ìƒì„± ì‹¤íŒ¨: {e}")

def _execute_query(query: str, params: Dict[str, Any], fetch_many: bool = False) -> List[Dict[str, Any]] | Dict[str, Any] | None:
    """DB ì¿¼ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ë‚´ë¶€ ìœ í‹¸ë¦¬í‹°."""
    if engine is None: 
        logger.warning("DB Engineì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None if not fetch_many else []
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query), params).mappings().all()
            
            processed_results = []
            for row in result:
                processed_row = dict(row)
                for key, value in processed_row.items():
                    if isinstance(value, (date, datetime)):
                        processed_row[key] = value.strftime("%Y-%m-%d")
                    elif isinstance(value, Decimal):
                        processed_row[key] = float(value) 
                processed_results.append(processed_row)
            
            if fetch_many: 
                return processed_results
            else: 
                return processed_results[0] if processed_results else None
    except Exception as e:
        logger.error(f"DB ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return None if not fetch_many else []

# ------------------------------------------------------------------
# ğŸ¯ [ìƒˆë¡œìš´ ìƒìˆ˜ ì •ì˜] ì •ì±… íŒŒì¼ê³¼ ì ìš© ì›”ì˜ ê·œì¹™ ë§¤í•‘ (YYYYMMDD_policy.pdf)
# ------------------------------------------------------------------
# ì •ì±… ë°°í¬ì¼(YYYYMMDD) ëª©ë¡. ì´ ë‚ ì§œì˜ ì •ì±…ì€ ë‹¤ìŒ ë‹¬ 1ì¼ ë³´ê³ ì„œì— ë°˜ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
POLICY_FILE_DATES = [
    "20250305",  # 2025ë…„ 4ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20241224",  # 2025ë…„ 1ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240724",  # 2024ë…„ 8ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240626",  # 2024ë…„ 7ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20240430",  # 2024ë…„ 5ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230830",  # 2023ë…„ 9ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230621",  # 2023ë…„ 7ì›” ë³´ê³ ì„œì— ë°˜ì˜
    "20230302",  # 2023ë…„ 4ì›” ë³´ê³ ì„œì— ë°˜ì˜
]

# ------------------------------------------------------------------
# ğŸ¯ [í•µì‹¬ í•¨ìˆ˜] ë³´ê³ ì„œ ì›”ì— í•´ë‹¹í•˜ëŠ” ì •ì±… íŒŒì¼ ì°¾ê¸°
# ------------------------------------------------------------------
def _find_policy_file_for_report(report_date_str: str) -> Optional[str]:
    """
    ë³´ê³ ì„œ ë‚ ì§œ(YYYY-MM-DD ë˜ëŠ” YYYY-MM)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, í•´ë‹¹ ì›”ì— ë°˜ì˜í•´ì•¼ í•  ì •ì±… íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    ë¡œì§:
    - ë³´ê³ ì„œ ì œê³µì¼(create_at)ì´ 2024-03-01ì´ë©´ â†’ 2024ë…„ 2ì›” ë¦¬í¬íŠ¸
    - 2024ë…„ 2ì›”ì— ë°œí‘œëœ ì •ì±…ì„ í¬í•¨
    - ì¦‰, report_date_strì˜ ì „ì›”(2ì›”)ì— ë°œí‘œëœ ì •ì±…ì„ ì°¾ìŒ
    """
    try:
        # ğŸ”§ ìˆ˜ì •: ì…ë ¥ í˜•ì‹ì— ê´€ê³„ì—†ì´ YYYY-MM-DD í˜•íƒœë¡œ ë³€í™˜
        if len(report_date_str) == 7:  # YYYY-MM í˜•ì‹
            report_date_str = report_date_str + "-01"
            
        # ë³´ê³ ì„œ ì œê³µì¼ (ì˜ˆ: 2024-03-01)
        report_delivery_date = datetime.strptime(report_date_str[:10], "%Y-%m-%d").replace(day=1)
        
        # ğŸ¯ [í•µì‹¬ ìˆ˜ì •]: ë³´ê³ ì„œ ëŒ€ìƒ ì›” = ì œê³µì¼ì˜ ì „ì›” (ì˜ˆ: 2024-02)
        report_target_month = report_delivery_date - relativedelta(months=1)

        # ì •ì±… íŒŒì¼ ë‚ ì§œ ëª©ë¡ì„ ì—­ìˆœìœ¼ë¡œ ìˆœíšŒ (ìµœì‹  ì •ì±…ë¶€í„° í™•ì¸)
        for policy_date_str in sorted(POLICY_FILE_DATES, reverse=True):
            policy_date = datetime.strptime(policy_date_str, "%Y%m%d").date()
            
            # ğŸ¯ [í•µì‹¬ ìˆ˜ì •]: ì •ì±… ë°œí‘œ ì›” (ì˜ˆ: 2024-02)
            policy_month = datetime(policy_date.year, policy_date.month, 1)
            
            # ì •ì±… ë°œí‘œ ì›” == ë³´ê³ ì„œ ëŒ€ìƒ ì›”ì´ë©´ í•´ë‹¹ ì •ì±… ì‚¬ìš©
            if policy_month == report_target_month:
                filename = f"{policy_date_str}_policy.pdf"
                full_path = os.path.join(POLICY_DIR, filename)
                
                if os.path.exists(full_path):
                    logger.info(f"RAG: {report_delivery_date.strftime('%Y-%m')} ì œê³µ ë¦¬í¬íŠ¸(ëŒ€ìƒì›”: {report_target_month.strftime('%Y-%m')})ì— {filename} ì •ì±… íŒŒì¼ ì§€ì •ë¨.")
                    return full_path
                else:
                    logger.warning(f"RAG: ì§€ì •ëœ ì •ì±… íŒŒì¼({filename})ì´ ë””ë ‰í† ë¦¬ì— ì—†ìŠµë‹ˆë‹¤. ({full_path})")
                    return None
        
        logger.info(f"RAG: {report_delivery_date.strftime('%Y-%m')} ì œê³µ ë¦¬í¬íŠ¸(ëŒ€ìƒì›”: {report_target_month.strftime('%Y-%m')})ì— ë°˜ì˜í•  ì •ì±… íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.error(f"ì •ì±… íŒŒì¼ ê²°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return None


# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ TOOL 0] ë²¡í„° DB ì¬êµ¬ì¶• ë° ì—…ë°ì´íŠ¸
# ------------------------------------------------------------------
# ... (ì£¼ì„ ì²˜ë¦¬ëœ api_rebuild_vector_db í•¨ìˆ˜ ìœ ì§€) ...


# ------------------------------------------------------------------
# ğŸ¯ [ì •ì±… ì„¹ì…˜ ì •ì˜] RAG ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ì„¹ì…˜ ëª©ë¡
# ------------------------------------------------------------------
# 1ì¥ì€ ë¬¸ì„œ ì „ì²´ ê°œì • ì´ë ¥ë§Œ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³ , 2ì¥ë¶€í„° ê²€ìƒ‰
POLICY_SECTIONS_TO_CHECK = [
    "2ì¥ ë‚˜. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ë‹´ë³´ì¸ì •ë¹„ìœ¨(LTV) ë³€ë™ ë° íŠ¹ë¡€ ì ìš©",
    "3ì¥ ë‹¤. ì£¼íƒë‹´ë³´ëŒ€ì¶œ ì´ë¶€ì±„ìƒí™˜ë¹„ìœ¨(DTI) ì ìš© ë° ë°°ì œ ê¸°ì¤€",
    "4ì¥ ë¼. ê³ ì•¡ ê°€ê³„ëŒ€ì¶œ DSR ì ìš© ê¸°ì¤€ ë° ì˜ˆì™¸ ì‚¬í•­",
    "5ì¥ ë§ˆ. ì£¼íƒê´€ë ¨ ë‹´ë³´ëŒ€ì¶œ ì·¨ê¸‰ ê´€ë ¨ ìœ ì˜ì‚¬í•­ ë° íŠ¹ë¡€ ëŒ€ì¶œ ì‹ ì„¤"
]


# ------------------------------------------------------------------
# ğŸ¯ RAG ê²€ìƒ‰ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë‹¨ì¼ íŒŒì¼ í•„í„°ë§ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •)
# ------------------------------------------------------------------
def _rag_similarity_search(query: str, k: int = 5, required_sources: Optional[List[str]] = None) -> str:
    """FAISS DBë¥¼ ë¡œë“œí•˜ì—¬ ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì§€ì •ëœ ì†ŒìŠ¤ íŒŒì¼ ëª©ë¡ì—ì„œë§Œ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""

    if not HUGGINGFACEHUB_API_TOKEN:
        return "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: HUGGINGFACEHUB_API_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
    current_model = HF_EMBEDDING_MODEL 
    logger.info(f"RAG: ì„ë² ë”© ëª¨ë¸ {current_model} ì‚¬ìš©.")

    try:
        embeddings = HuggingFaceEndpointEmbeddings(
            model=current_model,
            huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
        )
        
        db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        
        found_chunks = db.similarity_search(query, k=k * 4) 
        
        logger.info(f"RAG: ê²€ìƒ‰ì–´ '{query}'ë¡œ {len(found_chunks)}ê°œ ì²­í¬ ë°œê²¬ (required_sources: {required_sources})")
        
        context = []
        filtered_count = 0
        
        for idx, chunk in enumerate(found_chunks):
            source = chunk.metadata.get("source", "ì¶œì²˜ ë¯¸ìƒ")
            
            # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ ì²­í¬ì˜ source ì¶œë ¥
            if idx < 5:
                logger.info(f"RAG: ì²­í¬ {idx} - source: '{source}'")
            
            is_valid_source = not required_sources or any(req_src in source for req_src in required_sources)
            
            if not is_valid_source:
                if idx < 5:
                    logger.info(f"RAG: ì²­í¬ {idx} - í•„í„°ë§ë¨ (source ë¶ˆì¼ì¹˜)")
                continue

            if filtered_count < k:
                context.append(f"[ì¶œì²˜: {source}]\n{chunk.page_content}")
                filtered_count += 1
                if idx < 5:
                    logger.info(f"RAG: ì²­í¬ {idx} - í¬í•¨ë¨!")
            else:
                break

        logger.info(f"RAG: ìµœì¢… {filtered_count}ê°œ ì²­í¬ ë°˜í™˜ (ëª©í‘œ: {k}ê°œ)")
        
        if not context:
            source_info = f"ë¬¸ì„œ ëª©ë¡: {required_sources}" if required_sources else "ëª¨ë“  ë¬¸ì„œ"
            return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•´ {source_info}ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
        return "\n---\n".join(context)
    
    except Exception as e:
        logger.error(f"RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}", exc_info=True)
        return f"ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {type(e).__name__} - {e}"


# ------------------------------------------------------------------
# ğŸ¯ [ì‹ ê·œ í•¨ìˆ˜]: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ 100% íƒì§€ (ê°œì •/ì‹ ì„¤ ìœ ì—°ì„± ê°•í™”)
# ------------------------------------------------------------------
def _find_policies_by_marker_regex(context: str, target_date: Optional[str] = None) -> List[Dict[str, str]]:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ <ì‹ ì„¤ YYYY.M.D.> ë˜ëŠ” <ê°œì • YYYY.M.D.> ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì •ì±… êµ¬ë¬¸ì„ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ ë° ì •ê·œí™”.
    
    Args:
        context: RAG ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        target_date: í•„í„°ë§í•  ëª©í‘œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì§€ì • ì‹œ í•´ë‹¹ ë‚ ì§œì˜ ë³€ê²½ì‚¬í•­ë§Œ ë°˜í™˜
    """
    
    context_clean = re.sub(r'\[ì¶œì²˜:.*?\.pdf\]', '', context, flags=re.DOTALL)
    context_clean = re.sub(r'---\n', '', context_clean, flags=re.DOTALL)
    
    # ì •ê·œí‘œí˜„ì‹: ì¡°í•­ ë²ˆí˜¸ + ë‚´ìš© + <ì‹ ì„¤/ê°œì • ë‚ ì§œ> íŒ¨í„´ ì°¾ê¸°
    # ì˜ˆ: "21.(ì„ì°¨ë³´ì¦ê¸ˆë°˜í™˜ëª©ì ...) <ê°œì • 2024.7.24., 2024.12.24.>"
    # <ë³„í‘œ6><ì‹ ì„¤...> ê°™ì€ ë¬¸ì„œ ì „ì²´ ê°œì • ì´ë ¥ì€ ì œì™¸
    regex = r"(\d{1,3}\.[\s\S]{10,1000}?)<\s*(ì‹ ì„¤|ê°œì •)\s*([^>]+)>"
    
    matches = re.findall(regex, context_clean, re.DOTALL) 
    
    logger.info(f"RAG: ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­ ê²°ê³¼ {len(matches)}ê°œ ë°œê²¬")
    
    extracted_changes = []
    
    for policy_text, change_type, dates_str in matches:
        # <ë³„í‘œX> íŒ¨í„´ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸
        if re.search(r'<ë³„í‘œ\d+>', policy_text):
            logger.info(f"RAG: <ë³„í‘œ> íŒ¨í„´ ë°œê²¬ìœ¼ë¡œ ì œì™¸")
            continue
        
        # ë‚ ì§œ ë¬¸ìì—´ì—ì„œ ëª¨ë“  ë‚ ì§œ ì¶”ì¶œ
        date_pattern = r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        all_dates = re.findall(date_pattern, dates_str)
        
        if not all_dates:
            logger.warning(f"RAG: ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ - dates_str: '{dates_str}'")
            continue
        
        # ê°€ì¥ ìµœì‹  ë‚ ì§œ ì°¾ê¸° (ë§ˆì§€ë§‰ ë‚ ì§œê°€ ë³´í†µ ìµœì‹ )
        latest_date_tuple = all_dates[-1]
        year, month, day = latest_date_tuple
        
        try:
            effective_date = datetime(int(year), int(month), int(day)).strftime("%Y-%m-%d")
        except ValueError:
            logger.warning(f"RAG: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ - year: {year}, month: {month}, day: {day}")
            continue
        
        logger.info(f"RAG: ë°œê²¬ëœ ë³€ê²½ì‚¬í•­ - ë‚ ì§œ: {effective_date}, íƒ€ì…: {change_type}, ì¡°í•­: {policy_text[:50]}...")
        
        # target_dateê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ í¬í•¨
        if target_date:
            if effective_date != target_date:
                logger.info(f"RAG: ë‚ ì§œ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸ - effective_date: {effective_date}, target_date: {target_date}")
                continue
            else:
                logger.info(f"RAG: ë‚ ì§œ ì¼ì¹˜! í¬í•¨ - {effective_date}")
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        normalized_text = policy_text.strip()
        normalized_text = re.sub(r'\s{2,}', ' ', normalized_text)
        
        # ë§ˆì»¤ ì¶”ê°€
        full_text = f"{normalized_text} <{change_type} {dates_str}>"
        
        extracted_changes.append({
            "effective_date": effective_date,
            "policy_text": full_text
        })

    logger.info(f"RAG: ìµœì¢… ì¶”ì¶œëœ ë³€ê²½ì‚¬í•­ {len(extracted_changes)}ê°œ (target_date: {target_date})")
    return extracted_changes



# ------------------------------------------------------------------
# ğŸ¯ [REMOVED]: _generate_final_report_from_structured_data
# This function has been removed. The Agent will now handle policy report generation.
# ------------------------------------------------------------------


# ==============================================================================
# ë…ë¦½ Tool 1: ì†Œë¹„ ë°ì´í„° ë¶„ì„ ë° êµ°ì§‘ ìƒì„±
# ==============================================================================
@router.post(
    "/analyze_user_spending",
    summary="ì›”ë³„ ì†Œë¹„ ë°ì´í„° ë¹„êµ ë¶„ì„ ë° êµ°ì§‘ ìƒì„±",
    operation_id="analyze_user_spending_tool", 
    description="ë‘ ë‹¬ì¹˜ ì†Œë¹„ ë°ì´í„°(DataFrame Records)ë¥¼ ë°›ì•„ ì´ ì§€ì¶œ, Top 5 ì¹´í…Œê³ ë¦¬ë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. Agentê°€ ì´ ë°ì´í„°ë¥¼ ë³´ê³  ë³„ëª…ê³¼ ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_spending(
    consume_records: List[Dict[str, Any]] = Body(..., embed=True),
    member_data: Dict[str, Any] = Body(..., embed=False)
) -> dict:
    """ì†Œë¹„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. Agentê°€ ì´ ë°ì´í„°ë¡œ ë³„ëª…ê³¼ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""

    if not consume_records or len(consume_records) < 2:
        error_msg = "ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ìµœì†Œ 2ê°œì›” ë°ì´í„° ë¶€ì¡±" if consume_records else "ë¶„ì„í•  ì†Œë¹„ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤."
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": False, 
            "error": error_msg,
            "consume_analysis_summary": {},
            "spend_chart_json": json.dumps({})
        }
    
    try:
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ë° ì •ë ¬
        df_consume = pd.DataFrame(consume_records)
        
        # ğŸš¨ [Fix] Handle 'YYYY_MM' format from DB by replacing '_' with '-'
        if 'year_and_month' in df_consume.columns:
            df_consume['year_and_month'] = df_consume['year_and_month'].astype(str).str.replace('_', '-')
            
        df_consume['year_and_month'] = pd.to_datetime(df_consume['year_and_month'])
        df_consume = df_consume.sort_values(by='year_and_month', ascending=False)
        
        latest_data = df_consume.iloc[0] # ìµœì‹  ì›” ë°ì´í„°
        previous_data = df_consume.iloc[1]

        total_spend_latest = latest_data.get('total_spend', 0) or 0
        total_spend_prev = previous_data.get('total_spend', 0) or 0
        diff = total_spend_latest - total_spend_prev
        change_rate = (diff / total_spend_prev) * 100 if total_spend_prev else 0
        change_text = f"{diff:+,}ì› ({change_rate:.2f}%) ë³€ë™"

        # ğŸš¨ [ìˆ˜ì •] ì†Œë¶„ë¥˜(CAT2) ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ëŒ€ë¶„ë¥˜(CAT1) ì‚¬ìš©
        cat2_cols = [col for col in latest_data.index if col.startswith('CAT2_')]
        target_cols = cat2_cols if cat2_cols else [col for col in latest_data.index if col.startswith('CAT1_')]
        prefix = 'CAT2_' if cat2_cols else 'CAT1_'
        
        # Top 5 ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        latest_cats = df_consume.iloc[0][target_cols].sort_values(ascending=False).head(5) 
        
        # spend_chart_jsonì„ ìœ„í•œ ì „ì²´ ì¹´í…Œê³ ë¦¬ë³„ ê¸ˆì•¡ ê³„ì‚°
        chart_data_list = []
        for col in target_cols:
            amount = latest_data.get(col, 0) or 0
            if amount > 0:
                # ë¼ë²¨ ì •ì œ: ì ‘ë‘ì‚¬ ì œê±° ë° ì–¸ë”ë°”ë¥¼ ê³µë°±/ìŠ¬ë˜ì‹œë¡œ ë³€í™˜
                label = col.replace(prefix, '').replace('_', ' ').replace(' ', '/') 
                chart_data_list.append({
                    "category": label,
                    "amount": int(amount)
                })
        spend_chart_json = json.dumps(chart_data_list, ensure_ascii=False)
        
        # Top 5 ì¹´í…Œê³ ë¦¬ ì´ë¦„ê³¼ ê¸ˆì•¡
        top_5_categories = [col.replace(prefix, '').replace('_', ' ') for col in latest_cats.index]
        top_5_amounts = [int(latest_cats[col]) for col in latest_cats.index]
        
        # consume_analysis_summary êµ¬ì„± (Agentê°€ ì‚¬ìš©í•  ë°ì´í„°)
        consume_analysis_summary = {
            'latest_total_spend': int(total_spend_latest),
            'previous_total_spend': int(total_spend_prev),
            'spend_diff': int(diff),
            'change_rate': round(change_rate, 2),
            'total_change_diff': change_text,
            'top_5_categories': top_5_categories,
            'top_5_amounts': top_5_amounts,
            'member_info': member_data
        }
        
        return {
            "tool_name": "analyze_user_spending_tool", 
            "success": True, 
            "consume_analysis_summary": consume_analysis_summary,
            "spend_chart_json": spend_chart_json
        }

    except Exception as e:
        logger.error(f"ì†Œë¹„ ë¶„ì„ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"tool_name": "analyze_user_spending_tool", "success": False, "error": str(e)}

    
# ==============================================================================
# ë…ë¦½ Tool 2: ìµœì¢… 3ì¤„ ìš”ì•½ LLM Tool
# ==============================================================================
@router.post(
    "/generate_final_summary",
    summary="ìµœì¢… ë³´ê³ ì„œ 3ì¤„ ìš”ì•½ ìƒì„±",
    operation_id="generate_final_summary_llm", 
    description="í†µí•© ë³´ê³ ì„œ ë³¸ë¬¸ì„ ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_generate_final_summary(report_content: str = Body(..., embed=True)) -> dict:
    """
    [DEPRECATED] This tool now returns the report content as-is. 
    The Agent will handle summarization internally instead of calling this tool.
    """
    
    return {
        "tool_name": "generate_final_summary_llm", 
        "success": True, 
        "report_content": report_content,
        "message": "ì´ ë„êµ¬ëŠ” ë” ì´ìƒ LLMì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Agentê°€ ì§ì ‘ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }



# ==============================================================================
# ë…ë¦½ Tool 3: ì •ì±… ë³€ë™ ìë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„± íˆ´ (ğŸŒŸ ìµœì¢… ìˆ˜ì •)
# ==============================================================================
@router.post(
    "/check_and_report_policy_changes",
    summary="ë§¤ì›” ìë™ ì •ì±… ë³€ë™ ë¹„êµ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="check_and_report_policy_changes_tool", 
    description="ì‚¬ìš©ì ì…ë ¥ ì—†ì´, ì •ì˜ëœ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ë³€ë™ ì‚¬í•­ì„ í™•ì¸í•˜ê³  êµ¬ì¡°í™”ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_check_policy_changes(
    report_month_str: str = Body(..., embed=True) 
) -> dict:
    """ë§¤ì›” ì •ê¸° ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ ì •ì±… ë³€ë™ ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""
    
    # 1. ğŸ“… ë³´ê³ ì„œ ì›” ì •ê·œí™” (YYYY-MM ë˜ëŠ” YYYY-MM-DD ëª¨ë‘ ì²˜ë¦¬)
    try:
        if len(report_month_str) == 7 and report_month_str.count('-') == 1:  # "YYYY-MM" í˜•ì‹
            # YYYY-MM í˜•ì‹ì¼ ê²½ìš°, ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ '-01'ì„ ì¶”ê°€
            report_month_dt = datetime.strptime(report_month_str + "-01", "%Y-%m-%d")
        elif len(report_month_str) >= 10 and report_month_str.count('-') >= 2: # "YYYY-MM-DD" ì´ìƒ í˜•ì‹
            # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°”ë¡œ íŒŒì‹±
            report_month_dt = datetime.strptime(report_month_str[:10], "%Y-%m-%d")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤: {report_month_str}")
            
        # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ 'YYYYë…„ MMì›”' í˜•ì‹
        report_month_display = report_month_dt.strftime('%Yë…„ %mì›”')
        report_month_date = report_month_dt.date() # ë¹„êµìš© Date ê°ì²´
        
        # ğŸ¯ ì •ì±… íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜ì— ì „ë‹¬í•  YYYY-MM-DD í˜•ì‹
        report_date_for_search = report_month_date.strftime('%Y-%m-%d')
        
    except ValueError as e:
        logger.error(f"ë³´ê³ ì„œ ì›” íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": False, 
            "policy_changes": [],
            "error": f"ë³´ê³ ì„œ ì›” í˜•ì‹ ì˜¤ë¥˜: ì…ë ¥ëœ ë‚ ì§œ '{report_month_str}'ì„(ë¥¼) ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # 2. ğŸ¯ [í•µì‹¬ ìˆ˜ì •]: ë³´ê³ ì„œ ì›”ì— í•´ë‹¹í•˜ëŠ” ì •ì±… íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    # _find_policy_file_for_reportëŠ” YYYY-MM-DD í˜•íƒœë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    LATEST_POLICY_SOURCE = _find_policy_file_for_report(report_date_for_search)
    
    if not LATEST_POLICY_SOURCE:
        # ì •ì±… íŒŒì¼ì´ ì—†ìœ¼ë©´ ë³€ë™ ì—†ìŒìœ¼ë¡œ ê°„ì£¼
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": True, 
            "policy_changes": [],
            "report_month": report_month_display,
            "message": f"{report_month_display} ë³´ê³ ì„œ ê¸°ì¤€, í•´ë‹¹ ì›”ì— ë°˜ì˜í•  ì •ì±… ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."
        }
        
    REQUIRED_SOURCES = [LATEST_POLICY_SOURCE] 
    
    # 3. ìµœì‹  ì •ì±… íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ ë° ê²€ì¦
    try:
        file_name = Path(LATEST_POLICY_SOURCE).name 
        latest_policy_date_str = file_name.split('_')[0] # 'YYYYMMDD' ì¶”ì¶œ
        latest_policy_date = datetime.strptime(latest_policy_date_str, "%Y%m%d").date()
        
    except Exception as e:
        logger.error(f"ì •ì±… íŒŒì¼ ì´ë¦„ íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": False, 
            "policy_changes": [],
            "error": "ì •ì±… íŒŒì¼ ì´ë¦„ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
        }
    
    # ----------------------------------------------------------------------
    # 4. âš™ï¸ ì •ì±… ì„¹ì…˜ë³„ë¡œ RAG ê²€ìƒ‰ ì‹¤í–‰ (ì§€ì •ëœ íŒŒì¼ë§Œ ëŒ€ìƒ)
    # ----------------------------------------------------------------------
    full_context_list = []
    K_SEARCH = 15  # ê° ì„¹ì…˜ë‹¹ ê²€ìƒ‰í•  ì²­í¬ ìˆ˜
 
    
    for section_query in POLICY_SECTIONS_TO_CHECK:
        rag_context = _rag_similarity_search(
            query=section_query, 
            k=K_SEARCH, 
            required_sources=REQUIRED_SOURCES 
        )

        if "ğŸš¨ RAG ê²€ìƒ‰ ì‹¤íŒ¨" not in rag_context:
            full_context_list.append(rag_context)
        else:
             return {
                "tool_name": "check_and_report_policy_changes_tool", 
                "success": False, 
                "policy_changes": [],
                "error": rag_context
            }

    combined_context = "\n---\n".join(full_context_list)
    
    # 5. ğŸ“ ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ RAG ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì»¤ í¬í•¨ êµ¬ë¬¸ ì¶”ì¶œ
    # ì •ì±… íŒŒì¼ ë‚ ì§œë¥¼ target_dateë¡œ ì „ë‹¬í•˜ì—¬ í•´ë‹¹ ë‚ ì§œì˜ ë³€ê²½ì‚¬í•­ë§Œ í•„í„°ë§
    target_policy_date = latest_policy_date.strftime("%Y-%m-%d")
    structured_changes = _find_policies_by_marker_regex(combined_context, target_date=target_policy_date)
    
    
    # 6. ğŸ§© ì¶”ì¶œ ê²°ê³¼ ì²˜ë¦¬ (ë³€ë™ ì‚¬í•­ì´ ì—†ëŠ” ê²½ìš°)
    if not structured_changes:
        # ì •ì±… íŒŒì¼ì€ ìˆì—ˆìœ¼ë‚˜, í•´ë‹¹ íŒŒì¼ì—ì„œ ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì •ì±… ë³€ë™ ì‚¬í•­ì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°
        return {
            "tool_name": "check_and_report_policy_changes_tool", 
            "success": True, 
            "policy_changes": [],
            "report_month": report_month_display,
            "policy_date": latest_policy_date.strftime('%Yë…„ %mì›” %dì¼'),
            "message": f"{report_month_display} ë³´ê³ ì„œ ê¸°ì¤€, ìµœì‹  ì •ì±… ë¬¸ì„œ({latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')})ì— ì‹ ì„¤ ë˜ëŠ” ê°œì •ëœ ì •ì±… ë³€ë™ ì‚¬í•­ì€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }

    # 7. ğŸ¯ ìµœì¢… ì•„ì›ƒí’‹ êµ¬ì„± (Agentê°€ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±)
    return {
        "tool_name": "check_and_report_policy_changes_tool", 
        "success": True, 
        "policy_changes": structured_changes,
        "report_month": report_month_display,
        "policy_date": latest_policy_date.strftime('%Yë…„ %mì›” %dì¼')
    }


# ==============================================================================
# ë…ë¦½ Tool 4: ì†ìµ/ì§„ì²™ë„ ë¶„ì„ (ì™„ì„±)
# ==============================================================================
@router.post(
    "/analyze_investment_profit",
    summary="íˆ¬ì ìƒí’ˆ ì†ìµ/ì§„ì²™ë„ ë¶„ì„ + ê·¸ë˜í”„ ë°ì´í„° ìƒì„±",
    operation_id="analyze_investment_profit_tool", 
    description="ì˜ˆê¸ˆ, ì ê¸ˆ, í€ë“œì˜ ìˆ˜ìµë¥ ê³¼ ì§„ì²™ë„ë¥¼ ë¶„ì„í•˜ê³  ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def api_analyze_investment_profit(
    user_id: int = Body(..., embed=True),
    # products, monthly_data, fund_portfolio_data are now fetched internally
) -> dict:
    """
    ë³´ìœ  ìƒí’ˆ ëª©ë¡ê³¼ ì›”ë³„ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ DBì—ì„œ ì§ì ‘ ì¡°íšŒí•˜ì—¬ ì†ìµ ë°ì´í„° ë° ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    # 1. DBì—ì„œ ë°ì´í„° ì¡°íšŒ
    # (1) ë³´ìœ  ìƒí’ˆ ëª©ë¡ (my_products)
    products_query = "SELECT * FROM my_products WHERE user_id = :uid"
    products = _execute_query(products_query, {"uid": user_id}, fetch_many=True) or []

    # (2) ì›”ë³„ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° (monthly_simulation_report) - ìµœê·¼ 12ê°œì›”
    monthly_query = """
        SELECT * FROM monthly_simulation_report 
        WHERE user_id = :uid 
        ORDER BY year_and_month ASC
        LIMIT 12
    """
    monthly_data = _execute_query(monthly_query, {"uid": user_id}, fetch_many=True) or []

    # (3) í€ë“œ í¬íŠ¸í´ë¦¬ì˜¤ ìŠ¤ëƒ…ìƒ· (monthly_fund_portfolio_snapshot) - ìµœì‹  ì›”
    latest_month_query = "SELECT MAX(year_and_month) as max_month FROM monthly_fund_portfolio_snapshot WHERE user_id = :uid"
    latest_month_result = _execute_query(latest_month_query, {"uid": user_id}, fetch_many=False)
    
    fund_portfolio_data = []
    if latest_month_result and latest_month_result.get("max_month"):
        target_month = latest_month_result["max_month"]
        fund_query = """
            SELECT * FROM monthly_fund_portfolio_snapshot 
            WHERE user_id = :uid AND year_and_month = :month
        """
        fund_portfolio_data = _execute_query(fund_query, {"uid": user_id, "month": target_month}, fetch_many=True) or []
    
    total_principal = 0
    total_valuation = 0
    
    # 1. í˜„ì¬ ë³´ìœ  ìƒí’ˆ ì†ìµ ê³„ì‚° (my_product í…Œì´ë¸” ê¸°ì¤€)
    if products:
        for p in products:
            # payment_amount: íˆ¬ì ì›ê¸ˆ, current_value: í˜„ì¬ í‰ê°€ì•¡
            principal = p.get('payment_amount', 0) or 0
            valuation = p.get('current_value', 0) or 0
            
            # ë¬¸ìì—´ì¼ ê²½ìš° float ë³€í™˜
            if isinstance(principal, str): principal = float(principal)
            if isinstance(valuation, str): valuation = float(valuation)
            
            total_principal += principal
            total_valuation += valuation

    net_profit = total_valuation - total_principal
    profit_rate = (net_profit / total_principal) * 100 if total_principal else 0
    
    # 2. ê·¸ë˜í”„ 1: ì›”ë³„ ìˆ˜ìµë¥  ì¶”ì´ (monthly_simulation_report ê¸°ë°˜)
    trend_chart_data = []
    if monthly_data:
        for record in monthly_data:
            # total_return_rateëŠ” 0.05 ì²˜ëŸ¼ ì†Œìˆ˜ì ìœ¼ë¡œ ì €ì¥ë¨ -> 100 ê³±í•´ì„œ %ë¡œ ë³€í™˜
            fund_rate = float(record.get("total_return_rate", 0) or 0) * 100
            
            trend_chart_data.append({
                "month": record.get("year_and_month", ""),
                "deposit_rate": float(record.get("deposit_rate", 0) or 0),
                "savings_rate": float(record.get("savings_rate", 0) or 0),
                "fund_rate": round(fund_rate, 2)
            })
    
    trend_chart_json = json.dumps(trend_chart_data, ensure_ascii=False)
    
    # 3. ê·¸ë˜í”„ 2: í€ë“œ ìƒí’ˆë³„ ì†ìµ (monthly_fund_portfolio_snapshot ê¸°ë°˜)
    fund_comparison_data = []
    if fund_portfolio_data:
        for fund in fund_portfolio_data:
            invested = float(fund.get('invested_amount', 0) or 0)
            eval_amt = float(fund.get('eval_amount', 0) or 0)
            profit = eval_amt - invested
            
            fund_comparison_data.append({
                "name": fund.get('fund_product_name', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                "principal": int(invested),
                "valuation": int(eval_amt),
                "profit": int(profit)
            })
            
    fund_comparison_json = json.dumps(fund_comparison_data, ensure_ascii=False)
    
    return {
        "tool_name": "analyze_investment_profit_tool", 
        "success": True, 
        "total_principal": int(total_principal),
        "total_valuation": int(total_valuation),
        "net_profit": int(net_profit),
        "profit_rate": round(profit_rate, 2),
        "products_count": len(products) if products else 0,
        "trend_chart_json": trend_chart_json,
        "fund_comparison_json": fund_comparison_json
    }


# ==============================================================================
# ë…ë¦½ Tool 5: ì‚¬ìš©ì í”„ë¡œí•„ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„± - ğŸŒŸ ìµœì¢… ì•ˆì •í™”
# ==============================================================================
@router.post(
    "/analyze_user_profile_changes",
    summary="ì‚¬ìš©ì ê°œì¸ ì§€ìˆ˜ ë³€ë™ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±",
    operation_id="analyze_user_profile_changes_tool",
    description="ì§ì „ ë³´ê³ ì„œì™€ í˜„ì¬ DBì—ì„œ ì¡°íšŒí•œ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ë¥¼ ë¹„êµí•˜ê³ , LLMì„ í†µí•´ ë³€ë™ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
    response_model=dict,
)
async def analyze_user_profile_changes(
    current_data: Dict[str, Any] = Body(..., embed=True),
    previous_data: Dict[str, Any] = Body(..., embed=False)
) -> dict:
    """ì‚¬ìš©ìì˜ ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜ ë³€ë™ ë°ì´í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (LLM í˜¸ì¶œ ì œê±° - Agentê°€ ì²˜ë¦¬)"""
    
    # 1. ğŸ“Š ë°ì´í„° ë¹„êµ ë° ìš”ì•½
    change_raw_changes = []
    
    # ë¹„êµ ëŒ€ìƒ í•„ë“œ ë¦¬ìŠ¤íŠ¸
    fields_to_compare = [
        ('annual_salary', 'ì—°ë´‰'), 
        ('total_debt', 'ì´ ë¶€ì±„'), 
        ('credit_score', 'ì‹ ìš© ì ìˆ˜')
    ]

    is_first_report = all(v == 0 for k, v in previous_data.items() if k in ['annual_salary', 'total_debt', 'credit_score'])
    
    for field, name in fields_to_compare:
        current_value = current_data.get(field, 0) or 0
        previous_value = previous_data.get(field, 0) or 0
        
        # ğŸš¨ Decimal/Floatì´ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ìˆ˜ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        current_value = int(float(current_value))
        previous_value = int(float(previous_value))

        diff = current_value - previous_value
        
        # ğŸš¨ [ìˆ˜ì •]: ì²« ë³´ê³ ì„œê°€ ì•„ë‹ ë•Œë§Œ 0ì´ ì•„ë‹Œ ìœ ì˜ë¯¸í•œ ë³€ë™ì„ ë¹„êµ
        if not is_first_report and diff != 0:
            change_raw_changes.append(f"{name} ë³€ë™: {previous_value:,}ì› â†’ {current_value:,}ì› ({diff:+,}ì›)")
        # ğŸš¨ [ì¶”ê°€]: ì²« ë³´ê³ ì„œì´ê³ , í˜„ì¬ ë°ì´í„°ê°€ 0ì´ ì•„ë‹Œ ê²½ìš° í˜„ì¬ ìƒíƒœë§Œ ê¸°ë¡
        elif is_first_report and current_value != 0:
             change_raw_changes.append(f"ìµœì´ˆ ê¸°ë¡ {name}: {current_value:,}ì›")
    
    if not change_raw_changes:
        return {
            "tool_name": "analyze_user_profile_changes_tool",
            "success": True,
            "change_raw_changes": [],
            "is_first_report": is_first_report,
            "message": "ì§ì „ ë³´ê³ ì„œ ëŒ€ë¹„ ì£¼ìš” ê°œì¸ ê¸ˆìœµ ì§€í‘œ(ì—°ë´‰, ë¶€ì±„, ì‹ ìš© ì ìˆ˜)ì˜ ë³€ë™ ì‚¬í•­ì€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    return {
        "tool_name": "analyze_user_profile_changes_tool",
        "success": True,
        "change_raw_changes": change_raw_changes,
        "is_first_report": is_first_report
    }